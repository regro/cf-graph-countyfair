{
  "lm_eval.datasets": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.arithmetic": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.arithmetic.arithmetic": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.asdiv": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.asdiv.asdiv": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.coqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.coqa.coqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.drop": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.drop.drop": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.headqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.headqa.headqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.hendrycks_ethics": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.hendrycks_ethics.hendrycks_ethics": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.hendrycks_math": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.hendrycks_math.hendrycks_math": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.lambada_openai": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.lambada_openai.lambada_openai": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.logiqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.logiqa.logiqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.mutual": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.mutual.mutual": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.pile": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.pile.pile": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.quac": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.quac.quac": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.sat_analogies": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.sat_analogies.sat_analogies": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.triviaqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.triviaqa.triviaqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.unscramble": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.unscramble.unscramble": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.wikitext": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.datasets.wikitext.wikitext": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  }
}