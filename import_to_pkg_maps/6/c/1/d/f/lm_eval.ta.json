{
  "lm_eval.tasks": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.anli": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.arc": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.arithmetic": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.asdiv": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.blimp": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.cbt": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.coqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.drop": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.glue": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.gsm8k": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.headqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.hellaswag": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.hendrycks_ethics": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.hendrycks_math": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.hendrycks_test": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.lambada": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.lambada_cloze": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.lambada_multilingual": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.logiqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.mathqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.mc_taco": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.mutual": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.naturalqs": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.openbookqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.pile": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.piqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.prost": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.pubmedqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.qa4mre": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.qasper": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.quac": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.race": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.sat": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.sciq": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.squad": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.storycloze": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.superglue": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.swag": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.translation": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.triviaqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.truthfulqa": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.unscramble": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.webqs": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.wikitext": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.winogrande": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  },
  "lm_eval.tasks.wsc273": {
    "__set__": true,
    "elements": [
      "lm_eval"
    ]
  }
}