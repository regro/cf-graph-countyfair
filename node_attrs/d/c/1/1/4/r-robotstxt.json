{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "bot": {
      "automerge": true
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_name": "r-robotstxt",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Provides functions to download and parse 'robots.txt' files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to access specific resources on a domain."
    },
    "build": {
      "noarch": "generic",
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-robotstxt",
      "version": "0.7.15"
    },
    "requirements": {
      "build": [],
      "host": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ],
      "run": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "9021d98b44fdc8f25cc86a6e254ec4949b136d28f776789eaeb508ce52bd5ade",
      "url": [
        "https://cran.r-project.org/src/contrib/robotstxt_0.7.15.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.15.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('robotstxt')\""
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future",
        "r-future.apply",
        "r-httr",
        "r-magrittr",
        "r-spiderbar",
        "r-stringr"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future",
        "r-future.apply",
        "r-httr",
        "r-magrittr",
        "r-spiderbar",
        "r-stringr"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Provides functions to download and parse 'robots.txt' files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to access specific resources on a domain."
    },
    "build": {
      "noarch": "generic",
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-robotstxt",
      "version": "0.7.15"
    },
    "requirements": {
      "build": [],
      "host": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ],
      "run": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "9021d98b44fdc8f25cc86a6e254ec4949b136d28f776789eaeb508ce52bd5ade",
      "url": [
        "https://cran.r-project.org/src/contrib/robotstxt_0.7.15.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.15.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('robotstxt')\""
      ]
    }
  },
  "name": "r-robotstxt",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "r-robotstxt"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/r-robotstxt.json"
  },
  "raw_meta_yaml": "{% set version = \"0.7.15\" %}\n{% set posix = 'm2-' if win else '' %}\n\npackage:\n  name: r-robotstxt\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  url:\n    - {{ cran_mirror }}/src/contrib/robotstxt_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/robotstxt/robotstxt_{{ version }}.tar.gz\n  sha256: 9021d98b44fdc8f25cc86a6e254ec4949b136d28f776789eaeb508ce52bd5ade\n\nbuild:\n  number: 2\n  noarch: generic\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - {{ posix }}zip               # [win]\n    - cross-r-base {{ r_base }}    # [build_platform != target_platform]\n  host:\n    - r-base\n    - r-future >=1.6.2\n    - r-future.apply >=1.0.0\n    - r-httr >=1.0.0\n    - r-magrittr\n    - r-spiderbar >=0.2.0\n    - r-stringr >=1.0.0\n  run:\n    - r-base\n    - r-future >=1.6.2\n    - r-future.apply >=1.0.0\n    - r-httr >=1.0.0\n    - r-magrittr\n    - r-spiderbar >=0.2.0\n    - r-stringr >=1.0.0\n\ntest:\n  commands:\n    - $R -e \"library('robotstxt')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('robotstxt')\\\"\"  # [win]\n\nabout:\n  home: https://docs.ropensci.org/robotstxt/, https://github.com/ropensci/robotstxt\n  license: MIT\n  summary: Provides functions to download and parse 'robots.txt' files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, ...) are allowed to access specific resources on a domain.\n  license_family: MIT\n  license_file:\n    - '{{ environ[\"PREFIX\"] }}/lib/R/share/licenses/MIT'\n    - LICENSE\n\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n",
  "req": {
    "__set__": true,
    "elements": [
      "r-base",
      "r-future",
      "r-future.apply",
      "r-httr",
      "r-magrittr",
      "r-spiderbar",
      "r-stringr"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future",
        "r-future.apply",
        "r-httr",
        "r-magrittr",
        "r-spiderbar",
        "r-stringr"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future",
        "r-future.apply",
        "r-httr",
        "r-magrittr",
        "r-spiderbar",
        "r-stringr"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-future >=1.6.2",
        "r-future.apply >=1.0.0",
        "r-httr >=1.0.0",
        "r-magrittr",
        "r-spiderbar >=0.2.0",
        "r-stringr >=1.0.0"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": [
    "https://cran.r-project.org/src/contrib/robotstxt_0.7.15.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/robotstxt/robotstxt_0.7.15.tar.gz"
  ],
  "version": "0.7.15",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/r-robotstxt.json"
  }
}