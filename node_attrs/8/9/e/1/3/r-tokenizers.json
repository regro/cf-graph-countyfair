{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "bot": {
      "automerge": true
    },
    "build_platform": {
      "linux_aarch64": "linux_64",
      "linux_ppc64le": "linux_64",
      "osx_arm64": "osx_64"
    },
    "conda_build": {
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "provider": {
      "linux_aarch64": "default",
      "linux_ppc64le": "default",
      "win": "azure"
    },
    "test": "native_and_emulated"
  },
  "feedstock_name": "r-tokenizers",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\""
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\""
      ]
    }
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "linux_ppc64le_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\""
      ]
    }
  },
  "linux_ppc64le_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make",
        "m2w64_c_compiler_stub",
        "m2w64_c_stdlib_stub",
        "m2w64_cxx_compiler_stub",
        "filesystem",
        "sed",
        "coreutils",
        "zip"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\"",
        "\"%R%\" -e \"library('tokenizers')\""
      ]
    }
  },
  "name": "r-tokenizers",
  "osx_64_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\""
      ]
    }
  },
  "osx_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "osx_arm64_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('tokenizers')\""
      ]
    }
  },
  "osx_arm64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "outputs_names": {
    "__set__": true,
    "elements": [
      "r-tokenizers"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64",
    "linux_ppc64le",
    "osx_64",
    "osx_arm64",
    "win_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/r-tokenizers.json"
  },
  "raw_meta_yaml": "{% set version = \"0.3.0\" %}\n{% set posix = 'm2-' if win else '' %}\n\npackage:\n  name: r-tokenizers\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  fn: tokenizers_{{ version }}.tar.gz\n  url:\n    - {{ cran_mirror }}/src/contrib/tokenizers_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/tokenizers/tokenizers_{{ version }}.tar.gz\n  sha256: 24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29\n\nbuild:\n  number: 2\n  skip: true  # [win32]\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - cross-r-base {{ r_base }}  # [build_platform != target_platform]\n    - r-rcpp                     # [build_platform != target_platform]\n    - r-snowballc                # [build_platform != target_platform]\n    - r-stringi                  # [build_platform != target_platform]\n    - {{ compiler('c') }}        # [not win]\n    - {{ stdlib(\"c\") }}          # [not win]\n    - {{ compiler('cxx') }}      # [not win]\n    - {{ compiler('m2w64_c') }}        # [win]\n    - {{ stdlib(\"m2w64_c\") }}          # [win]\n    - {{ compiler('m2w64_cxx') }}        # [win]\n    - {{ posix }}filesystem        # [win]\n    - {{ posix }}make\n    - {{ posix }}sed               # [win]\n    - {{ posix }}coreutils         # [win]\n    - {{ posix }}zip               # [win]\n  host:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n  run:\n    - r-base\n    - r-rcpp >=0.12.3\n    - r-snowballc >=0.5.1\n    - r-stringi >=1.0.1\n\ntest:\n  commands:\n    - $R -e \"library('tokenizers')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('tokenizers')\\\"\"  # [win]\n\nabout:\n  home: https://lincolnmullen.com/software/tokenizers/\n  license: MIT\n  summary: \"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for\\\n    \\ splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. \"\n  license_family: MIT\n\n  license_file:\n    - {{ environ[\"PREFIX\"] }}/lib/R/share/licenses/MIT\n    - LICENSE\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n",
  "req": {
    "__set__": true,
    "elements": [
      "c_compiler_stub",
      "c_stdlib_stub",
      "coreutils",
      "cxx_compiler_stub",
      "filesystem",
      "m2w64_c_compiler_stub",
      "m2w64_c_stdlib_stub",
      "m2w64_cxx_compiler_stub",
      "make",
      "r-base",
      "r-rcpp",
      "r-snowballc",
      "r-stringi",
      "sed",
      "zip"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "coreutils",
        "cxx_compiler_stub",
        "filesystem",
        "m2w64_c_compiler_stub",
        "m2w64_c_stdlib_stub",
        "m2w64_cxx_compiler_stub",
        "make",
        "sed",
        "zip"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "coreutils",
        "cxx_compiler_stub",
        "filesystem",
        "m2w64_c_compiler_stub",
        "m2w64_c_stdlib_stub",
        "m2w64_cxx_compiler_stub",
        "make",
        "sed",
        "zip"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": [
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
  ],
  "version": "0.3.0",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/r-tokenizers.json"
  },
  "win_64_meta_yaml": {
    "about": {
      "home": "https://lincolnmullen.com/software/tokenizers/",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": [
        "/lib/R/share/licenses/MIT",
        "LICENSE"
      ],
      "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'. "
    },
    "build": {
      "number": "2",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-tokenizers",
      "version": "0.3.0"
    },
    "requirements": {
      "build": [
        "m2w64_c_compiler_stub",
        "m2w64_c_stdlib_stub",
        "m2w64_cxx_compiler_stub",
        "filesystem",
        "make",
        "sed",
        "coreutils",
        "zip"
      ],
      "host": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ],
      "run": [
        "r-base",
        "r-rcpp >=0.12.3",
        "r-snowballc >=0.5.1",
        "r-stringi >=1.0.1"
      ]
    },
    "schema_version": 0,
    "source": {
      "fn": "tokenizers_0.3.0.tar.gz",
      "sha256": "24571e4642a1a2d9f4f4c7a363b514eece74788d59c09012a5190ee718a91c29",
      "url": [
        "https://cran.r-project.org/src/contrib/tokenizers_0.3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "\"%R%\" -e \"library('tokenizers')\""
      ]
    }
  },
  "win_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "coreutils",
        "filesystem",
        "m2w64_c_compiler_stub",
        "m2w64_c_stdlib_stub",
        "m2w64_cxx_compiler_stub",
        "make",
        "sed",
        "zip"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base",
        "r-rcpp",
        "r-snowballc",
        "r-stringi"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  }
}