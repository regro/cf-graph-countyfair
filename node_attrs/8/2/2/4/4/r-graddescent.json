{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_r_base4.4.yaml": "cdt_name:\n- conda\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ncran_mirror:\n- https://cran.r-project.org\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\npin_run_as_build:\n  r-base:\n    min_pin: x.x\n    max_pin: x.x\nr_base:\n- '4.4'\ntarget_platform:\n- linux-64\n",
  "conda-forge.yml": {
    "bot": {
      "automerge": true
    },
    "conda_build": {
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "ebf6aefea1f28af6441bbc9c38e86fd3656fe593",
  "feedstock_hash_ts": 1758742329,
  "feedstock_name": "r-graddescent",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://github.com/drizzersilverberg/gradDescentR",
      "license": "GPL-2.0-or-later",
      "license_family": "GPL2",
      "license_file": [
        "/lib/R/share/licenses/GPL-2",
        "LICENSE"
      ],
      "summary": "An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. The variants of gradient descent algorithm are : Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically. Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning. Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning. Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient. Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time. Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information. Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination."
    },
    "build": {
      "noarch": "generic",
      "number": "4",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-graddescent",
      "version": "3.0"
    },
    "requirements": {
      "build": [],
      "host": [
        "r-base"
      ],
      "run": [
        "r-base"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "e9acd505aea624c381cfe95839460a2dad4df4f4937007d65c65e2c4a5ddafcb",
      "url": [
        "https://cran.r-project.org/src/contrib/gradDescent_3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/gradDescent/gradDescent_3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('gradDescent')\""
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://github.com/drizzersilverberg/gradDescentR",
      "license": "GPL-2.0-or-later",
      "license_family": "GPL2",
      "license_file": [
        "/lib/R/share/licenses/GPL-2",
        "LICENSE"
      ],
      "summary": "An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. The variants of gradient descent algorithm are : Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically. Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning. Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning. Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient. Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time. Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information. Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination."
    },
    "build": {
      "noarch": "generic",
      "number": "4",
      "rpaths": [
        "lib/R/lib/",
        "lib/"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/r"
      ]
    },
    "package": {
      "name": "r-graddescent",
      "version": "3.0"
    },
    "requirements": {
      "build": [],
      "host": [
        "r-base"
      ],
      "run": [
        "r-base"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "e9acd505aea624c381cfe95839460a2dad4df4f4937007d65c65e2c4a5ddafcb",
      "url": [
        "https://cran.r-project.org/src/contrib/gradDescent_3.0.tar.gz",
        "https://cran.r-project.org/src/contrib/Archive/gradDescent/gradDescent_3.0.tar.gz"
      ]
    },
    "test": {
      "commands": [
        "$R -e \"library('gradDescent')\""
      ]
    }
  },
  "name": "r-graddescent",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "r-graddescent"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/r-graddescent.json"
  },
  "raw_meta_yaml": "{% set version = '3.0' %}\n{% set posix = 'm2-' if win else '' %}\n\npackage:\n  name: r-graddescent\n  version: {{ version|replace(\"-\", \"_\") }}\n\nsource:\n  url:\n    - {{ cran_mirror }}/src/contrib/gradDescent_{{ version }}.tar.gz\n    - {{ cran_mirror }}/src/contrib/Archive/gradDescent/gradDescent_{{ version }}.tar.gz\n  sha256: e9acd505aea624c381cfe95839460a2dad4df4f4937007d65c65e2c4a5ddafcb\n\nbuild:\n  number: 4\n  noarch: generic\n  rpaths:\n    - lib/R/lib/\n    - lib/\n\nrequirements:\n  build:\n    - {{ posix }}zip               # [win]\n    - cross-r-base {{ r_base }}    # [build_platform != target_platform]\n  host:\n    - r-base\n  run:\n    - r-base\n\ntest:\n  commands:\n    - $R -e \"library('gradDescent')\"           # [not win]\n    - \"\\\"%R%\\\" -e \\\"library('gradDescent')\\\"\"  # [win]\n\nabout:\n  home: https://github.com/drizzersilverberg/gradDescentR\n  license: GPL-2.0-or-later\n  summary: 'An implementation of various learning algorithms based on Gradient Descent for dealing\n    with regression tasks. The variants of gradient descent algorithm are : Mini-Batch\n    Gradient Descent (MBGD), which is an optimization to use training data partially\n    to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization\n    to use a random data in learning to reduce the computation load drastically. Stochastic\n    Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step\n    to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up\n    gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization\n    to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based\n    algorithm that accumulate previous cost to do adaptive learning. Adadelta, which\n    is a gradient-descent-based algorithm that use hessian approximation to do adaptive\n    learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad\n    and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based\n    algorithm that mean and variance moment to do adaptive learning. Stochastic Variance\n    Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates\n    the process toward converging by reducing the gradient. Semi Stochastic Gradient\n    Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates\n    the process toward converging by choosing one of the gradients at a time. Stochastic\n    Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly\n    SVRG to accelerates the process toward converging by accumulated stochastic information.\n    Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical\n    variant algorithm to accelerates the process toward converging provides a possibility\n    of earlier termination.'\n  license_family: GPL2\n  license_file:\n    - '{{ environ[\"PREFIX\"] }}/lib/R/share/licenses/GPL-2'\n    - LICENSE\n\nextra:\n  recipe-maintainers:\n    - conda-forge/r\n\n# Package: gradDescent\n# Maintainer: Lala Septem Riza <lala.s.riza@upi.edu>\n# Type: Package\n# Title: Gradient Descent for Regression Tasks\n# Version: 3.0\n# URL: https://github.com/drizzersilverberg/gradDescentR\n# Date: 2018-01-03\n# Author: Galih Praja Wijaya, Dendi Handian, Imam Fachmi Nasrulloh, Lala Septem Riza, Rani Megasari, Enjun Junaeti\n# Description: An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. The variants of gradient descent algorithm are : Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically. Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning. Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning. Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient. Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time. Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information. Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination.\n# License: GPL (>= 2) | file LICENSE\n# RoxygenNote: 6.0.1\n# NeedsCompilation: no\n# Packaged: 2018-01-23 14:06:03 UTC; GalihPW\n# Repository: CRAN\n# Date/Publication: 2018-01-25 13:33:54 UTC\n",
  "req": {
    "__set__": true,
    "elements": [
      "r-base"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "r-base"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": [
    "https://cran.r-project.org/src/contrib/gradDescent_3.0.tar.gz",
    "https://cran.r-project.org/src/contrib/Archive/gradDescent/gradDescent_3.0.tar.gz"
  ],
  "version": "3.0",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/r-graddescent.json"
  }
}