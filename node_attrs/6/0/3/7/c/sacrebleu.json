{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_.yaml": "cdt_name:\n- conda\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\npython_min:\n- '3.9'\n",
  "conda-forge.yml": {
    "bot": {
      "inspection": "update-grayskull"
    },
    "conda_build": {
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "d29c0fd87586a3a768bec3071e6f5118c713e3db",
  "feedstock_hash_ts": 1735995629,
  "feedstock_name": "sacrebleu",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "SacreBLEU (Post, 2018) provides hassle-free computation of shareable,\ncomparable, and reproducible BLEU scores. Inspired by Rico Sennrich's\nmulti-bleu-detok.perl, it produces the official WMT scores but works with\nplain text. It also knows all the standard test sets and handles\ndownloading, processing, and tokenization for you.\n",
      "home": "https://github.com/mjpost/sacrebleu",
      "license": "Apache-2.0",
      "license_file": "LICENSE.txt",
      "summary": "Reference BLEU implementation that auto-downloads test sets and reports a version string to facilitate cross-lab comparisons"
    },
    "build": {
      "entry_points": [
        "sacrebleu = sacrebleu.sacrebleu:main"
      ],
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv"
    },
    "extra": {
      "recipe-maintainers": [
        "hmaarrfk"
      ]
    },
    "package": {
      "name": "sacrebleu",
      "version": "2.5.1"
    },
    "requirements": {
      "host": [
        "python 3.9",
        "pip",
        "setuptools_scm",
        "setuptools"
      ],
      "run": [
        "python >=3.9",
        "typing",
        "portalocker",
        "regex",
        "tabulate >=0.8.9",
        "numpy >=1.17",
        "colorama",
        "lxml"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "1a088cc1c74ffaff0759c3191a85db09eecfa7a52e09be244e319d8d64e2fb11",
      "url": "https://pypi.org/packages/source/s/sacrebleu/sacrebleu-2.5.1.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "sacrebleu"
      ],
      "requires": [
        "python 3.9",
        "pip"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools",
        "setuptools_scm"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "colorama",
        "lxml",
        "numpy",
        "portalocker",
        "python",
        "regex",
        "tabulate",
        "typing"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "SacreBLEU (Post, 2018) provides hassle-free computation of shareable,\ncomparable, and reproducible BLEU scores. Inspired by Rico Sennrich's\nmulti-bleu-detok.perl, it produces the official WMT scores but works with\nplain text. It also knows all the standard test sets and handles\ndownloading, processing, and tokenization for you.\n",
      "home": "https://github.com/mjpost/sacrebleu",
      "license": "Apache-2.0",
      "license_file": "LICENSE.txt",
      "summary": "Reference BLEU implementation that auto-downloads test sets and reports a version string to facilitate cross-lab comparisons"
    },
    "build": {
      "entry_points": [
        "sacrebleu = sacrebleu.sacrebleu:main"
      ],
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv"
    },
    "extra": {
      "recipe-maintainers": [
        "hmaarrfk"
      ]
    },
    "package": {
      "name": "sacrebleu",
      "version": "2.5.1"
    },
    "requirements": {
      "host": [
        "python 3.9",
        "pip",
        "setuptools_scm",
        "setuptools"
      ],
      "run": [
        "python >=3.9",
        "typing",
        "portalocker",
        "regex",
        "tabulate >=0.8.9",
        "numpy >=1.17",
        "colorama",
        "lxml"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "1a088cc1c74ffaff0759c3191a85db09eecfa7a52e09be244e319d8d64e2fb11",
      "url": "https://pypi.org/packages/source/s/sacrebleu/sacrebleu-2.5.1.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "sacrebleu"
      ],
      "requires": [
        "python 3.9",
        "pip"
      ]
    }
  },
  "name": "sacrebleu",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "sacrebleu"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/sacrebleu.json"
  },
  "raw_meta_yaml": "{% set version = \"2.5.1\" %}\n\npackage:\n  name: sacrebleu\n  version: {{ version }}\n\nsource:\n  url: https://pypi.org/packages/source/s/sacrebleu/sacrebleu-{{ version }}.tar.gz\n  sha256: 1a088cc1c74ffaff0759c3191a85db09eecfa7a52e09be244e319d8d64e2fb11\n\nbuild:\n  noarch: python\n  number: 0\n  script: {{ PYTHON }} -m pip install . -vv\n  entry_points:\n    - sacrebleu = sacrebleu.sacrebleu:main\n\nrequirements:\n  host:\n    - python {{ python_min }}\n    - pip\n    - setuptools_scm\n    - setuptools\n  run:\n    - python >={{ python_min }}\n    - typing\n    - portalocker\n    - regex\n    - tabulate >=0.8.9\n    - numpy >=1.17\n    - colorama\n    - lxml\n\ntest:\n  requires:\n    - python {{ python_min }}\n    - pip\n  imports:\n    - sacrebleu\n  commands:\n    - pip check\n\nabout:\n  home: https://github.com/mjpost/sacrebleu\n  license: Apache-2.0\n  license_file: LICENSE.txt\n  summary: Reference BLEU implementation that auto-downloads test sets and reports a version string to facilitate cross-lab comparisons\n\n  description: |\n    SacreBLEU (Post, 2018) provides hassle-free computation of shareable,\n    comparable, and reproducible BLEU scores. Inspired by Rico Sennrich's\n    multi-bleu-detok.perl, it produces the official WMT scores but works with\n    plain text. It also knows all the standard test sets and handles\n    downloading, processing, and tokenization for you.\n\nextra:\n  recipe-maintainers:\n    - hmaarrfk\n",
  "req": {
    "__set__": true,
    "elements": [
      "colorama",
      "lxml",
      "numpy",
      "pip",
      "portalocker",
      "python",
      "regex",
      "setuptools",
      "setuptools_scm",
      "tabulate",
      "typing"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools",
        "setuptools_scm"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "colorama",
        "lxml",
        "numpy",
        "portalocker",
        "python",
        "regex",
        "tabulate",
        "typing"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.9",
        "setuptools",
        "setuptools_scm"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "colorama",
        "lxml",
        "numpy >=1.17",
        "portalocker",
        "python >=3.9",
        "regex",
        "tabulate >=0.8.9",
        "typing"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.9"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/s/sacrebleu/sacrebleu-2.5.1.tar.gz",
  "version": "2.5.1",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/sacrebleu.json"
  }
}