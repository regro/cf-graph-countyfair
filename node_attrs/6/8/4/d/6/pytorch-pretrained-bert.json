{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_python3.10.____cpython.yaml": "cdt_name:\n- conda\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\npin_run_as_build:\n  python:\n    min_pin: x.x\n    max_pin: x.x\npython:\n- 3.10.* *_cpython\npytorch:\n- '2.4'\ntarget_platform:\n- linux-64\n",
  "conda-forge.yml": {
    "conda_build": {
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "8d35df76e32e84ef735a33610f14b18c81566535",
  "feedstock_hash_ts": 1739249611,
  "feedstock_name": "pytorch-pretrained-bert",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "This repository contains op-for-op PyTorch reimplementations, pre-trained\nmodels and fine-tuning examples for:\n  - Google's BERT model,\n  - OpenAI's GPT model,\n  - Google/CMU's Transformer-XL model, and\n  - OpenAI's GPT-2 model.\nThese implementations have been tested on several datasets (see the\nexamples) and should match the performances of the associated TensorFlow\nimplementations (e.g. ~91 F1 on SQuAD for BERT, ~88 F1 on RocStories for\nOpenAI GPT and ~18.3 perplexity on WikiText 103 for the Transformer-XL).\n",
      "home": "https://github.com/huggingface/pytorch-pretrained-BERT",
      "license": "Apache-2.0",
      "license_family": "Apache",
      "license_file": "LICENSE",
      "summary": "PyTorch version of Google AI BERT model with script to load Google pre-trained models"
    },
    "build": {
      "entry_points": [
        "pytorch-pretrained-bert = pytorch_pretrained_bert.__main__:main",
        "pytorch_pretrained_bert = pytorch_pretrained_bert.__main__:main"
      ],
      "number": "2",
      "script": "PYTHON -m pip install . --no-deps -vv"
    },
    "extra": {
      "recipe-maintainers": [
        "CurtLH",
        "sodre"
      ]
    },
    "package": {
      "name": "pytorch-pretrained-bert",
      "version": "0.6.2"
    },
    "requirements": {
      "host": [
        "python",
        "pip",
        "setuptools"
      ],
      "run": [
        "boto3",
        "numpy",
        "python",
        "regex",
        "requests",
        "tqdm"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "9cf7c6221e854071b9844f2a9a581e05a24777351618c010493d9c76601c6747",
      "url": "https://pypi.org/packages/source/p/pytorch-pretrained-bert/pytorch_pretrained_bert-0.6.2.tar.gz"
    },
    "test": {
      "imports": [
        "pytorch_pretrained_bert"
      ],
      "requires": [
        "pytorch"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "boto3",
        "numpy",
        "python",
        "regex",
        "requests",
        "tqdm"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pytorch"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "This repository contains op-for-op PyTorch reimplementations, pre-trained\nmodels and fine-tuning examples for:\n  - Google's BERT model,\n  - OpenAI's GPT model,\n  - Google/CMU's Transformer-XL model, and\n  - OpenAI's GPT-2 model.\nThese implementations have been tested on several datasets (see the\nexamples) and should match the performances of the associated TensorFlow\nimplementations (e.g. ~91 F1 on SQuAD for BERT, ~88 F1 on RocStories for\nOpenAI GPT and ~18.3 perplexity on WikiText 103 for the Transformer-XL).\n",
      "home": "https://github.com/huggingface/pytorch-pretrained-BERT",
      "license": "Apache-2.0",
      "license_family": "Apache",
      "license_file": "LICENSE",
      "summary": "PyTorch version of Google AI BERT model with script to load Google pre-trained models"
    },
    "build": {
      "entry_points": [
        "pytorch-pretrained-bert = pytorch_pretrained_bert.__main__:main",
        "pytorch_pretrained_bert = pytorch_pretrained_bert.__main__:main"
      ],
      "number": "2",
      "script": "PYTHON -m pip install . --no-deps -vv"
    },
    "extra": {
      "recipe-maintainers": [
        "CurtLH",
        "sodre"
      ]
    },
    "package": {
      "name": "pytorch-pretrained-bert",
      "version": "0.6.2"
    },
    "requirements": {
      "host": [
        "python",
        "pip",
        "setuptools"
      ],
      "run": [
        "boto3",
        "numpy",
        "python",
        "regex",
        "requests",
        "tqdm"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "9cf7c6221e854071b9844f2a9a581e05a24777351618c010493d9c76601c6747",
      "url": "https://pypi.org/packages/source/p/pytorch-pretrained-bert/pytorch_pretrained_bert-0.6.2.tar.gz"
    },
    "test": {
      "imports": [
        "pytorch_pretrained_bert"
      ],
      "requires": [
        "pytorch"
      ]
    }
  },
  "name": "pytorch-pretrained-bert",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "pytorch-pretrained-bert"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/pytorch-pretrained-bert.json"
  },
  "raw_meta_yaml": "{% set name = \"pytorch-pretrained-bert\" %}\n{% set version = \"0.6.2\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/{{ name.replace('-','_') }}-{{ version }}.tar.gz\n  sha256: 9cf7c6221e854071b9844f2a9a581e05a24777351618c010493d9c76601c6747\n\nbuild:\n  skip: True  # [not linux or py<35]\n  number: 2\n  script: \"{{ PYTHON }} -m pip install . --no-deps -vv\"\n  entry_points:\n    - pytorch-pretrained-bert = pytorch_pretrained_bert.__main__:main\n    - pytorch_pretrained_bert = pytorch_pretrained_bert.__main__:main\n\nrequirements:\n  host:\n    - python\n    - pip\n    - setuptools\n  run:\n    - boto3\n    - numpy\n    - python\n    - regex\n    - requests\n    - tqdm\n\ntest:\n  requires:\n    - pytorch\n  imports:\n    - pytorch_pretrained_bert\n\nabout:\n  home: https://github.com/huggingface/pytorch-pretrained-BERT\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: 'PyTorch version of Google AI BERT model with script to load Google pre-trained models'\n  description: |\n    This repository contains op-for-op PyTorch reimplementations, pre-trained\n    models and fine-tuning examples for:\n      - Google's BERT model,\n      - OpenAI's GPT model,\n      - Google/CMU's Transformer-XL model, and\n      - OpenAI's GPT-2 model.\n    These implementations have been tested on several datasets (see the\n    examples) and should match the performances of the associated TensorFlow\n    implementations (e.g. ~91 F1 on SQuAD for BERT, ~88 F1 on RocStories for\n    OpenAI GPT and ~18.3 perplexity on WikiText 103 for the Transformer-XL).\n\nextra:\n  recipe-maintainers:\n    - CurtLH\n    - sodre\n",
  "req": {
    "__set__": true,
    "elements": [
      "boto3",
      "numpy",
      "pip",
      "python",
      "regex",
      "requests",
      "setuptools",
      "tqdm"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "boto3",
        "numpy",
        "python",
        "regex",
        "requests",
        "tqdm"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pytorch"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "boto3",
        "numpy",
        "python",
        "regex",
        "requests",
        "tqdm"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pytorch"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/p/pytorch-pretrained-bert/pytorch_pretrained_bert-0.6.2.tar.gz",
  "version": "0.6.2",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/pytorch-pretrained-bert.json"
  }
}