{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_c_stdlib_version2.28cuda_compiler_version12.9.yaml": "c_compiler:\n- gcc\nc_compiler_version:\n- '14'\nc_stdlib:\n- sysroot\nc_stdlib_version:\n- '2.28'\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ncuda_compiler:\n- cuda-nvcc\ncuda_compiler_version:\n- '12.9'\ncxx_compiler:\n- gxx\ncxx_compiler_version:\n- '14'\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\nnccl:\n- '2'\ntarget_platform:\n- linux-64\nzip_keys:\n- - c_compiler_version\n  - cxx_compiler_version\n  - c_stdlib_version\n  - cuda_compiler_version\n",
  "conda-forge.yml": {
    "azure": {
      "free_disk_space": true
    },
    "bot": {
      "automerge": true
    },
    "build_platform": {
      "linux_aarch64": "linux_64",
      "linux_ppc64le": "linux_64"
    },
    "conda_build": {
      "error_overlinking": true,
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "provider": {
      "linux_aarch64": "default",
      "linux_ppc64le": "default"
    }
  },
  "feedstock_hash": "c8d2ee5e31f96d5047a2712f752b52d8bbdfff01",
  "feedstock_hash_ts": 1766550898,
  "feedstock_name": "nccl",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
      "dev_url": "https://github.com/NVIDIA/nccl",
      "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
      "home": "https://developer.nvidia.com/nccl",
      "license": "BSD-3-Clause",
      "license_family": "BSD",
      "license_file": "LICENSE.txt",
      "summary": "Optimized primitives for collective multi-GPU communication"
    },
    "build": {
      "ignore_run_exports": [
        "cuda-version"
      ],
      "number": "0",
      "run_exports": [
        "nccl"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/cuda",
        "jakirkham",
        "leofang"
      ]
    },
    "package": {
      "name": "nccl",
      "version": "2.29.2.1"
    },
    "requirements": {
      "build": [
        "c_stdlib_stub",
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "make"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-version 13.0"
      ],
      "run": [
        "cuda-version"
      ]
    },
    "schema_version": 0,
    "source": {
      "patches": [
        "0001-use-conda-ar-not-system.patch"
      ],
      "sha256": "063e20649c4cfa01e789b4dc73514dbb5d73f9518e426823dab53316415e071b",
      "url": "https://github.com/NVIDIA/nccl/archive/v2.29.2-1.tar.gz"
    },
    "test": {
      "commands": [
        "test -f \"${PREFIX}/include/nccl.h\"",
        "test -f \"${PREFIX}/lib/libnccl.so\"",
        "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-version"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
      "dev_url": "https://github.com/NVIDIA/nccl",
      "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
      "home": "https://developer.nvidia.com/nccl",
      "license": "BSD-3-Clause",
      "license_family": "BSD",
      "license_file": "LICENSE.txt",
      "summary": "Optimized primitives for collective multi-GPU communication"
    },
    "build": {
      "ignore_run_exports": [
        "cuda-version"
      ],
      "number": "0",
      "run_exports": [
        "nccl"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/cuda",
        "jakirkham",
        "leofang"
      ]
    },
    "package": {
      "name": "nccl",
      "version": "2.29.2.1"
    },
    "requirements": {
      "build": [
        "c_stdlib_stub",
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "make"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-version 13.0"
      ],
      "run": [
        "cuda-version"
      ]
    },
    "schema_version": 0,
    "source": {
      "patches": [
        "0001-use-conda-ar-not-system.patch"
      ],
      "sha256": "063e20649c4cfa01e789b4dc73514dbb5d73f9518e426823dab53316415e071b",
      "url": "https://github.com/NVIDIA/nccl/archive/v2.29.2-1.tar.gz"
    },
    "test": {
      "commands": [
        "test -f \"${PREFIX}/include/nccl.h\"",
        "test -f \"${PREFIX}/lib/libnccl.so\"",
        "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
      ]
    }
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-version"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
      "dev_url": "https://github.com/NVIDIA/nccl",
      "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
      "home": "https://developer.nvidia.com/nccl",
      "license": "BSD-3-Clause",
      "license_family": "BSD",
      "license_file": "LICENSE.txt",
      "summary": "Optimized primitives for collective multi-GPU communication"
    },
    "build": {
      "ignore_run_exports": [
        "cuda-version"
      ],
      "number": "0",
      "run_exports": [
        "nccl"
      ]
    },
    "extra": {
      "recipe-maintainers": [
        "conda-forge/cuda",
        "jakirkham",
        "leofang"
      ]
    },
    "package": {
      "name": "nccl",
      "version": "2.29.2.1"
    },
    "requirements": {
      "build": [
        "c_stdlib_stub",
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "make"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-version 13.0"
      ],
      "run": [
        "cuda-version"
      ]
    },
    "schema_version": 0,
    "source": {
      "patches": [
        "0001-use-conda-ar-not-system.patch"
      ],
      "sha256": "063e20649c4cfa01e789b4dc73514dbb5d73f9518e426823dab53316415e071b",
      "url": "https://github.com/NVIDIA/nccl/archive/v2.29.2-1.tar.gz"
    },
    "test": {
      "commands": [
        "test -f \"${PREFIX}/include/nccl.h\"",
        "test -f \"${PREFIX}/lib/libnccl.so\"",
        "test ! -f \"${PREFIX}/lib/libnccl_static.a\""
      ]
    }
  },
  "name": "nccl",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "nccl"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/nccl.json"
  },
  "raw_meta_yaml": "{% set name = \"nccl\" %}\n{% set version = \"2.29.2-1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version|replace(\"-\", \".\") }}\n\nsource:\n  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}.tar.gz\n  sha256: 063e20649c4cfa01e789b4dc73514dbb5d73f9518e426823dab53316415e071b\n  patches:\n    - 0001-use-conda-ar-not-system.patch\n\nbuild:\n  number: 0\n  skip: true  # [(not linux) or cuda_compiler_version in (undefined, \"None\")]\n  ignore_run_exports:\n    # ignore `cuda-version` constraint in CUDA 12+ as this supports CUDA Enhanced Compatibility.\n    - cuda-version\n  run_exports:\n    # xref: https://github.com/NVIDIA/nccl/issues/218\n    - {{ pin_subpackage(name, max_pin=\"x\") }}\n\nrequirements:\n  build:\n    - {{ stdlib(\"c\") }}\n    - {{ compiler(\"c\") }}\n    - {{ compiler(\"cxx\") }}\n    - {{ compiler(\"cuda\") }}\n    - make\n  host:\n    - cuda-version {{ cuda_compiler_version }}\n  run:\n    - {{ pin_compatible(\"cuda-version\", min_pin=\"x\", max_pin=\"x\") }}\n\ntest:\n  commands:\n    - test -f \"${PREFIX}/include/nccl.h\"\n    - test -f \"${PREFIX}/lib/libnccl.so\"\n    - test ! -f \"${PREFIX}/lib/libnccl_static.a\"\n\nabout:\n  home: https://developer.nvidia.com/nccl\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  summary: Optimized primitives for collective multi-GPU communication\n\n  description: |\n    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\n    and multi-node collective communication primitives that are performance\n    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\n    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to\n    achieve high bandwidth over PCIe and NVLink high-speed interconnect.\n\n  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html\n  dev_url: https://github.com/NVIDIA/nccl\n\nextra:\n  recipe-maintainers:\n    - conda-forge/cuda\n    - jakirkham\n    - leofang\n",
  "req": {
    "__set__": true,
    "elements": [
      "c_compiler_stub",
      "c_stdlib_stub",
      "cuda-version",
      "cuda_compiler_stub",
      "cxx_compiler_stub",
      "make"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "make"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-version 12.9",
        "cuda-version 13.0"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": "https://github.com/NVIDIA/nccl/archive/v2.29.2-1.tar.gz",
  "version": "2.29.2.1",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/nccl.json"
  }
}