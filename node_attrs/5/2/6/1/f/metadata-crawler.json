{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "bot": {
      "automerge": true
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_name": "metadata-crawler",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "Harvest, normalise, and index climate / earth-system metadata from **POSIX**,\n**S3/MinIO**, and **OpenStack Swift** using configurable **DRS dialects**\n(CMIP6, CMIP5, CORDEX, …). Output to a temporary **catalogue** (JSONLines)\nand then **index** into systems such as **Solr** or **MongoDB**.\nConfiguration is **TOML** with inheritance, templating, and computed rules.\n",
      "dev_url": "https://github.com/freva-org/metadata-crawler",
      "home": "https://github.com/freva-org/metadata-crawler",
      "license": "BSD-3-Clause",
      "license_file": "LICENSE",
      "summary": "Crawl, extract and push climate metadata for indexing."
    },
    "build": {
      "entry_points": [
        "metadata-crawler = metadata_crawler.cli:cli",
        "mdc = metadata_crawler.cli:cli"
      ],
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "mo-dkrz.",
        "antarcticrainforest"
      ]
    },
    "package": {
      "name": "metadata-crawler",
      "version": "2510.0.0"
    },
    "requirements": {
      "host": [
        "python 3.11",
        "flit-core >=3.2",
        "pip"
      ],
      "run": [
        "python >=3.11",
        "aiohttp",
        "appdirs",
        "anyio",
        "ciso8601",
        "fsspec",
        "diskcache",
        "s3fs",
        "jinja2",
        "intake",
        "intake-xarray",
        "intake-esm",
        "pandas",
        "python-dateutil",
        "numpy",
        "orjson",
        "pyarrow",
        "h5netcdf",
        "pydantic",
        "rich",
        "rich-argparse",
        "tomli",
        "tomlkit",
        "typing_extensions",
        "zarr",
        "xarray",
        "httpx",
        "uvloop",
        "motor"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "1342132cbafdb329fd2e7796bfe07191d5cd24d0ed02b5690a0ba97c8db3a6a9",
      "url": "https://pypi.org/packages/source/m/metadata-crawler/metadata_crawler-2510.0.0.tar.gz"
    },
    "test": {
      "commands": [
        "pip check",
        "metadata-crawler --help",
        "mdc --help"
      ],
      "imports": [
        "metadata_crawler"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "flit-core",
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "anyio",
        "appdirs",
        "ciso8601",
        "diskcache",
        "fsspec",
        "h5netcdf",
        "httpx",
        "intake",
        "intake-esm",
        "intake-xarray",
        "jinja2",
        "motor",
        "numpy",
        "orjson",
        "pandas",
        "pyarrow",
        "pydantic",
        "python",
        "python-dateutil",
        "rich",
        "rich-argparse",
        "s3fs",
        "tomli",
        "tomlkit",
        "typing_extensions",
        "uvloop",
        "xarray",
        "zarr"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "Harvest, normalise, and index climate / earth-system metadata from **POSIX**,\n**S3/MinIO**, and **OpenStack Swift** using configurable **DRS dialects**\n(CMIP6, CMIP5, CORDEX, …). Output to a temporary **catalogue** (JSONLines)\nand then **index** into systems such as **Solr** or **MongoDB**.\nConfiguration is **TOML** with inheritance, templating, and computed rules.\n",
      "dev_url": "https://github.com/freva-org/metadata-crawler",
      "home": "https://github.com/freva-org/metadata-crawler",
      "license": "BSD-3-Clause",
      "license_file": "LICENSE",
      "summary": "Crawl, extract and push climate metadata for indexing."
    },
    "build": {
      "entry_points": [
        "metadata-crawler = metadata_crawler.cli:cli",
        "mdc = metadata_crawler.cli:cli"
      ],
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "mo-dkrz.",
        "antarcticrainforest"
      ]
    },
    "package": {
      "name": "metadata-crawler",
      "version": "2510.0.0"
    },
    "requirements": {
      "host": [
        "python 3.11",
        "flit-core >=3.2",
        "pip"
      ],
      "run": [
        "python >=3.11",
        "aiohttp",
        "appdirs",
        "anyio",
        "ciso8601",
        "fsspec",
        "diskcache",
        "s3fs",
        "jinja2",
        "intake",
        "intake-xarray",
        "intake-esm",
        "pandas",
        "python-dateutil",
        "numpy",
        "orjson",
        "pyarrow",
        "h5netcdf",
        "pydantic",
        "rich",
        "rich-argparse",
        "tomli",
        "tomlkit",
        "typing_extensions",
        "zarr",
        "xarray",
        "httpx",
        "uvloop",
        "motor"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "1342132cbafdb329fd2e7796bfe07191d5cd24d0ed02b5690a0ba97c8db3a6a9",
      "url": "https://pypi.org/packages/source/m/metadata-crawler/metadata_crawler-2510.0.0.tar.gz"
    },
    "test": {
      "commands": [
        "pip check",
        "metadata-crawler --help",
        "mdc --help"
      ],
      "imports": [
        "metadata_crawler"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "name": "metadata-crawler",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "metadata-crawler"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/metadata-crawler.json"
  },
  "raw_meta_yaml": "{% set name = \"metadata-crawler\" %}\n{% set version = \"2510.0.0\" %}\n{% set python_min = 3.11 %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/metadata_crawler-{{ version }}.tar.gz\n  sha256: 1342132cbafdb329fd2e7796bfe07191d5cd24d0ed02b5690a0ba97c8db3a6a9\n\nbuild:\n  entry_points:\n    - metadata-crawler = metadata_crawler.cli:cli\n    - mdc = metadata_crawler.cli:cli\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n  number: 0\n\nrequirements:\n  host:\n    - python {{ python_min }}\n    - flit-core >=3.2\n    - pip\n  run:\n    - python >={{ python_min }}\n    - aiohttp\n    - appdirs\n    - anyio\n    - ciso8601\n    - fsspec\n    - diskcache\n    - s3fs\n    - jinja2\n    - intake\n    - intake-xarray\n    - intake-esm\n    - pandas\n    - python-dateutil\n    - numpy\n    - orjson\n    - pyarrow\n    - h5netcdf\n    - pydantic\n    - rich\n    - rich-argparse\n    - tomli\n    - tomlkit\n    - typing_extensions\n    - zarr\n    - xarray\n    - httpx\n    - uvloop\n    - motor\n\ntest:\n  imports:\n    - metadata_crawler\n  commands:\n    - pip check\n    - metadata-crawler --help\n    - mdc --help\n  requires:\n    - pip\n\nabout:\n  summary: Crawl, extract and push climate metadata for indexing.\n  dev_url: https://github.com/freva-org/metadata-crawler\n  home: https://github.com/freva-org/metadata-crawler\n  description: |\n    Harvest, normalise, and index climate / earth-system metadata from **POSIX**,\n    **S3/MinIO**, and **OpenStack Swift** using configurable **DRS dialects**\n    (CMIP6, CMIP5, CORDEX, …). Output to a temporary **catalogue** (JSONLines)\n    and then **index** into systems such as **Solr** or **MongoDB**.\n    Configuration is **TOML** with inheritance, templating, and computed rules.\n  license: BSD-3-Clause\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - mo-dkrz.\n    - antarcticrainforest\n",
  "req": {
    "__set__": true,
    "elements": [
      "aiohttp",
      "anyio",
      "appdirs",
      "ciso8601",
      "diskcache",
      "flit-core",
      "fsspec",
      "h5netcdf",
      "httpx",
      "intake",
      "intake-esm",
      "intake-xarray",
      "jinja2",
      "motor",
      "numpy",
      "orjson",
      "pandas",
      "pip",
      "pyarrow",
      "pydantic",
      "python",
      "python-dateutil",
      "rich",
      "rich-argparse",
      "s3fs",
      "tomli",
      "tomlkit",
      "typing_extensions",
      "uvloop",
      "xarray",
      "zarr"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "flit-core",
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "anyio",
        "appdirs",
        "ciso8601",
        "diskcache",
        "fsspec",
        "h5netcdf",
        "httpx",
        "intake",
        "intake-esm",
        "intake-xarray",
        "jinja2",
        "motor",
        "numpy",
        "orjson",
        "pandas",
        "pyarrow",
        "pydantic",
        "python",
        "python-dateutil",
        "rich",
        "rich-argparse",
        "s3fs",
        "tomli",
        "tomlkit",
        "typing_extensions",
        "uvloop",
        "xarray",
        "zarr"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "flit-core >=3.2",
        "pip",
        "python 3.11"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "anyio",
        "appdirs",
        "ciso8601",
        "diskcache",
        "fsspec",
        "h5netcdf",
        "httpx",
        "intake",
        "intake-esm",
        "intake-xarray",
        "jinja2",
        "motor",
        "numpy",
        "orjson",
        "pandas",
        "pyarrow",
        "pydantic",
        "python >=3.11",
        "python-dateutil",
        "rich",
        "rich-argparse",
        "s3fs",
        "tomli",
        "tomlkit",
        "typing_extensions",
        "uvloop",
        "xarray",
        "zarr"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/m/metadata-crawler/metadata_crawler-2510.0.0.tar.gz",
  "version": "2510.0.0",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/metadata-crawler.json"
  }
}