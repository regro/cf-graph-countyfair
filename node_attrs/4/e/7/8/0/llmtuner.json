{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "bot": {
      "automerge": true,
      "inspection": "update-grayskull"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "e5b872324e431fc40ddcd5821b4cc470120d8299",
  "feedstock_hash_ts": 1750112833,
  "feedstock_name": "llmtuner",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://github.com/hiyouga/LLaMA-Factory",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Easy-to-use LLM fine-tuning framework"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "shaowei-su",
        "jan-janssen"
      ]
    },
    "package": {
      "name": "llmtuner",
      "version": "0.9.3"
    },
    "requirements": {
      "host": [
        "python 3.9",
        "setuptools >=61.0",
        "pip"
      ],
      "run": [
        "omegaconf",
        "librosa",
        "tokenizers >=0.19.0,<=0.21.1",
        "av",
        "numpy <2.0.0",
        "pandas >=2.0.0",
        "tyro <0.9.0",
        "pyyaml",
        "packaging",
        "python >=3.9",
        "pytorch >=1.13.1",
        "transformers >=4.45.0,<=4.52.4,!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0",
        "datasets >=2.16.0,<=3.6.0",
        "accelerate >=0.34.0,<=1.7.0",
        "peft >=0.14.0,<=0.15.2",
        "trl >=0.8.6,<=0.9.6",
        "gradio >=4.38.0,<=5.31.0",
        "scipy",
        "einops",
        "sentencepiece",
        "protobuf",
        "uvicorn",
        "pydantic <=2.10.6",
        "fastapi",
        "sse-starlette",
        "matplotlib-base >=3.7.0",
        "fire",
        "galore-torch",
        "tiktoken",
        "eval-type-backport"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "bf4afcf34f3c94ecbf56c8ebc1d2576719897b12a1b48f7b83cac24bfc595c48",
      "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.3.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "llamafactory"
      ],
      "requires": [
        "pip",
        "python 3.9"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate",
        "av",
        "datasets",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio",
        "librosa",
        "matplotlib-base",
        "numpy",
        "omegaconf",
        "packaging",
        "pandas",
        "peft",
        "protobuf",
        "pydantic",
        "python",
        "pytorch",
        "pyyaml",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers",
        "transformers",
        "trl",
        "tyro",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://github.com/hiyouga/LLaMA-Factory",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Easy-to-use LLM fine-tuning framework"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "shaowei-su",
        "jan-janssen"
      ]
    },
    "package": {
      "name": "llmtuner",
      "version": "0.9.3"
    },
    "requirements": {
      "host": [
        "python 3.9",
        "setuptools >=61.0",
        "pip"
      ],
      "run": [
        "omegaconf",
        "librosa",
        "tokenizers >=0.19.0,<=0.21.1",
        "av",
        "numpy <2.0.0",
        "pandas >=2.0.0",
        "tyro <0.9.0",
        "pyyaml",
        "packaging",
        "python >=3.9",
        "pytorch >=1.13.1",
        "transformers >=4.45.0,<=4.52.4,!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0",
        "datasets >=2.16.0,<=3.6.0",
        "accelerate >=0.34.0,<=1.7.0",
        "peft >=0.14.0,<=0.15.2",
        "trl >=0.8.6,<=0.9.6",
        "gradio >=4.38.0,<=5.31.0",
        "scipy",
        "einops",
        "sentencepiece",
        "protobuf",
        "uvicorn",
        "pydantic <=2.10.6",
        "fastapi",
        "sse-starlette",
        "matplotlib-base >=3.7.0",
        "fire",
        "galore-torch",
        "tiktoken",
        "eval-type-backport"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "bf4afcf34f3c94ecbf56c8ebc1d2576719897b12a1b48f7b83cac24bfc595c48",
      "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.3.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "llamafactory"
      ],
      "requires": [
        "pip",
        "python 3.9"
      ]
    }
  },
  "name": "llmtuner",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "llmtuner"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/llmtuner.json"
  },
  "raw_meta_yaml": "{% set name = \"llmtuner\" %}\n{% set version = \"0.9.3\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://github.com/hiyouga/LLaMA-Factory/archive/v{{ version }}.tar.gz\n  sha256: bf4afcf34f3c94ecbf56c8ebc1d2576719897b12a1b48f7b83cac24bfc595c48\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n  number: 0\n\nrequirements:\n  host:\n    - python {{ python_min }}\n    - setuptools >=61.0\n    - pip\n  run:\n    - omegaconf\n    - librosa\n    - tokenizers >=0.19.0,<=0.21.1\n    - av\n    - numpy <2.0.0\n    - pandas >=2.0.0\n    - tyro <0.9.0\n    - pyyaml\n    - packaging\n    - python >={{ python_min }}\n    - pytorch >=1.13.1\n    - transformers >=4.45.0,<=4.52.4,!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0\n    - datasets >=2.16.0,<=3.6.0\n    - accelerate >=0.34.0,<=1.7.0\n    - peft >=0.14.0,<=0.15.2\n    - trl >=0.8.6,<=0.9.6\n    - gradio >=4.38.0,<=5.31.0\n    - scipy\n    - einops\n    - sentencepiece\n    - protobuf\n    - uvicorn\n    - pydantic <=2.10.6\n    - fastapi\n    - sse-starlette\n    - matplotlib-base >=3.7.0\n    - fire\n    - galore-torch\n    - tiktoken\n    - eval-type-backport\n\ntest:\n  imports:\n    - llamafactory\n  commands:\n    - pip check\n  requires:\n    - pip\n    - python {{ python_min }}\n\nabout:\n  home: https://github.com/hiyouga/LLaMA-Factory\n  summary: Easy-to-use LLM fine-tuning framework\n  license: Apache-2.0\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - shaowei-su\n    - jan-janssen\n",
  "req": {
    "__set__": true,
    "elements": [
      "accelerate",
      "av",
      "datasets",
      "einops",
      "eval-type-backport",
      "fastapi",
      "fire",
      "galore-torch",
      "gradio",
      "librosa",
      "matplotlib-base",
      "numpy",
      "omegaconf",
      "packaging",
      "pandas",
      "peft",
      "pip",
      "protobuf",
      "pydantic",
      "python",
      "pytorch",
      "pyyaml",
      "scipy",
      "sentencepiece",
      "setuptools",
      "sse-starlette",
      "tiktoken",
      "tokenizers",
      "transformers",
      "trl",
      "tyro",
      "uvicorn"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate",
        "av",
        "datasets",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio",
        "librosa",
        "matplotlib-base",
        "numpy",
        "omegaconf",
        "packaging",
        "pandas",
        "peft",
        "protobuf",
        "pydantic",
        "python",
        "pytorch",
        "pyyaml",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers",
        "transformers",
        "trl",
        "tyro",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.9",
        "setuptools >=61.0"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate >=0.34.0,<=1.7.0",
        "av",
        "datasets >=2.16.0,<=3.6.0",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio >=4.38.0,<=5.31.0",
        "librosa",
        "matplotlib-base >=3.7.0",
        "numpy <2.0.0",
        "omegaconf",
        "packaging",
        "pandas >=2.0.0",
        "peft >=0.14.0,<=0.15.2",
        "protobuf",
        "pydantic <=2.10.6",
        "python >=3.9",
        "pytorch >=1.13.1",
        "pyyaml",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers >=0.19.0,<=0.21.1",
        "transformers >=4.45.0,<=4.52.4,!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0",
        "trl >=0.8.6,<=0.9.6",
        "tyro <0.9.0",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.9"
      ]
    }
  },
  "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.3.tar.gz",
  "version": "0.9.3",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/llmtuner.json"
  }
}