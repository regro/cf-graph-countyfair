{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_.yaml": "channel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\npython_min:\n- '3.10'\n",
  "conda-forge.yml": {
    "bot": {
      "automerge": true,
      "inspection": "update-grayskull"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "f2d2a8de47a6ed41933a87f022b17331f9562880",
  "feedstock_hash_ts": 1767251593,
  "feedstock_name": "llmtuner",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://github.com/hiyouga/LLaMA-Factory",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Easy-to-use LLM fine-tuning framework"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "shaowei-su",
        "jan-janssen"
      ]
    },
    "package": {
      "name": "llmtuner",
      "version": "0.9.4"
    },
    "requirements": {
      "host": [
        "python 3.11",
        "hatchling",
        "pip"
      ],
      "run": [
        "hf-transfer",
        "modelscope",
        "safetensors",
        "torchaudio >=2.4.0",
        "torchdata >=0.10.0,<=0.11.0",
        "torchvision >=0.19.0",
        "omegaconf",
        "librosa",
        "tokenizers >=0.19.0,<=0.21.1",
        "av",
        "numpy",
        "pandas",
        "tyro <0.9.0",
        "pyyaml",
        "packaging",
        "python >=3.11",
        "pytorch >=2.4.0",
        "transformers >=4.51.0,<=4.57.1,!=4.52.0,!=4.57.0",
        "datasets >=2.16.0,<=4.0.0",
        "accelerate >=1.3.0,<=1.11.0",
        "peft >=0.14.0,<=0.17.1",
        "trl >=0.18.0,<=0.24.0",
        "gradio >=4.38.0,<=5.50.0",
        "scipy",
        "einops",
        "sentencepiece",
        "protobuf",
        "uvicorn",
        "pydantic",
        "fastapi",
        "sse-starlette",
        "matplotlib-base >=3.7.0",
        "fire",
        "galore-torch",
        "tiktoken",
        "eval-type-backport"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "2ce772945da1c7c28964e23d8bae429222749a4e9df47191e2159e3f72f638fd",
      "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.4.tar.gz"
    },
    "test": {
      "imports": [
        "llamafactory"
      ],
      "requires": [
        "python 3.11"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "hatchling",
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate",
        "av",
        "datasets",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio",
        "hf-transfer",
        "librosa",
        "matplotlib-base",
        "modelscope",
        "numpy",
        "omegaconf",
        "packaging",
        "pandas",
        "peft",
        "protobuf",
        "pydantic",
        "python",
        "pytorch",
        "pyyaml",
        "safetensors",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers",
        "torchaudio",
        "torchdata",
        "torchvision",
        "transformers",
        "trl",
        "tyro",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "python"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://github.com/hiyouga/LLaMA-Factory",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Easy-to-use LLM fine-tuning framework"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "shaowei-su",
        "jan-janssen"
      ]
    },
    "package": {
      "name": "llmtuner",
      "version": "0.9.4"
    },
    "requirements": {
      "host": [
        "python 3.11",
        "hatchling",
        "pip"
      ],
      "run": [
        "hf-transfer",
        "modelscope",
        "safetensors",
        "torchaudio >=2.4.0",
        "torchdata >=0.10.0,<=0.11.0",
        "torchvision >=0.19.0",
        "omegaconf",
        "librosa",
        "tokenizers >=0.19.0,<=0.21.1",
        "av",
        "numpy",
        "pandas",
        "tyro <0.9.0",
        "pyyaml",
        "packaging",
        "python >=3.11",
        "pytorch >=2.4.0",
        "transformers >=4.51.0,<=4.57.1,!=4.52.0,!=4.57.0",
        "datasets >=2.16.0,<=4.0.0",
        "accelerate >=1.3.0,<=1.11.0",
        "peft >=0.14.0,<=0.17.1",
        "trl >=0.18.0,<=0.24.0",
        "gradio >=4.38.0,<=5.50.0",
        "scipy",
        "einops",
        "sentencepiece",
        "protobuf",
        "uvicorn",
        "pydantic",
        "fastapi",
        "sse-starlette",
        "matplotlib-base >=3.7.0",
        "fire",
        "galore-torch",
        "tiktoken",
        "eval-type-backport"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "2ce772945da1c7c28964e23d8bae429222749a4e9df47191e2159e3f72f638fd",
      "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.4.tar.gz"
    },
    "test": {
      "imports": [
        "llamafactory"
      ],
      "requires": [
        "python 3.11"
      ]
    }
  },
  "name": "llmtuner",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "llmtuner"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/llmtuner.json"
  },
  "raw_meta_yaml": "{% set name = \"llmtuner\" %}\n{% set version = \"0.9.4\" %}\n{% set python_min = \"3.11\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://github.com/hiyouga/LLaMA-Factory/archive/v{{ version }}.tar.gz\n  sha256: 2ce772945da1c7c28964e23d8bae429222749a4e9df47191e2159e3f72f638fd\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n  number: 0\n\nrequirements:\n  host:\n    - python {{ python_min }}\n    - hatchling\n    - pip\n  run:\n    - hf-transfer\n    - modelscope\n    - safetensors\n    - torchaudio >=2.4.0\n    - torchdata >=0.10.0,<=0.11.0\n    - torchvision >=0.19.0\n    - omegaconf\n    - librosa\n    - tokenizers >=0.19.0,<=0.21.1\n    - av\n    - numpy\n    - pandas\n    - tyro <0.9.0\n    - pyyaml\n    - packaging\n    - python >={{ python_min }}\n    - pytorch >=2.4.0\n    - transformers >=4.51.0,<=4.57.1,!=4.52.0,!=4.57.0\n    - datasets >=2.16.0,<=4.0.0\n    - accelerate >=1.3.0,<=1.11.0\n    - peft >=0.14.0,<=0.17.1\n    - trl >=0.18.0,<=0.24.0\n    - gradio >=4.38.0,<=5.50.0\n    - scipy\n    - einops\n    - sentencepiece\n    - protobuf\n    - uvicorn\n    - pydantic\n    - fastapi\n    - sse-starlette\n    - matplotlib-base >=3.7.0\n    - fire\n    - galore-torch\n    - tiktoken\n    - eval-type-backport\n\ntest:\n  imports:\n    - llamafactory\n  # commands:\n  #   - pip check\n  requires:\n  #   - pip\n    - python {{ python_min }}\n\nabout:\n  home: https://github.com/hiyouga/LLaMA-Factory\n  summary: Easy-to-use LLM fine-tuning framework\n  license: Apache-2.0\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - shaowei-su\n    - jan-janssen\n",
  "req": {
    "__set__": true,
    "elements": [
      "accelerate",
      "av",
      "datasets",
      "einops",
      "eval-type-backport",
      "fastapi",
      "fire",
      "galore-torch",
      "gradio",
      "hatchling",
      "hf-transfer",
      "librosa",
      "matplotlib-base",
      "modelscope",
      "numpy",
      "omegaconf",
      "packaging",
      "pandas",
      "peft",
      "pip",
      "protobuf",
      "pydantic",
      "python",
      "pytorch",
      "pyyaml",
      "safetensors",
      "scipy",
      "sentencepiece",
      "sse-starlette",
      "tiktoken",
      "tokenizers",
      "torchaudio",
      "torchdata",
      "torchvision",
      "transformers",
      "trl",
      "tyro",
      "uvicorn"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "hatchling",
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate",
        "av",
        "datasets",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio",
        "hf-transfer",
        "librosa",
        "matplotlib-base",
        "modelscope",
        "numpy",
        "omegaconf",
        "packaging",
        "pandas",
        "peft",
        "protobuf",
        "pydantic",
        "python",
        "pytorch",
        "pyyaml",
        "safetensors",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers",
        "torchaudio",
        "torchdata",
        "torchvision",
        "transformers",
        "trl",
        "tyro",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "python"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "hatchling",
        "pip",
        "python 3.11"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "accelerate >=1.3.0,<=1.11.0",
        "av",
        "datasets >=2.16.0,<=4.0.0",
        "einops",
        "eval-type-backport",
        "fastapi",
        "fire",
        "galore-torch",
        "gradio >=4.38.0,<=5.50.0",
        "hf-transfer",
        "librosa",
        "matplotlib-base >=3.7.0",
        "modelscope",
        "numpy",
        "omegaconf",
        "packaging",
        "pandas",
        "peft >=0.14.0,<=0.17.1",
        "protobuf",
        "pydantic",
        "python >=3.11",
        "pytorch >=2.4.0",
        "pyyaml",
        "safetensors",
        "scipy",
        "sentencepiece",
        "sse-starlette",
        "tiktoken",
        "tokenizers >=0.19.0,<=0.21.1",
        "torchaudio >=2.4.0",
        "torchdata >=0.10.0,<=0.11.0",
        "torchvision >=0.19.0",
        "transformers >=4.51.0,<=4.57.1,!=4.52.0,!=4.57.0",
        "trl >=0.18.0,<=0.24.0",
        "tyro <0.9.0",
        "uvicorn"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "python 3.11"
      ]
    }
  },
  "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.9.4.tar.gz",
  "version": "0.9.4",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/llmtuner.json"
  }
}