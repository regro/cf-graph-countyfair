{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "azure": {
      "free_disk_space": true
    },
    "build_platform": {
      "linux_aarch64": "linux_64",
      "osx_arm64": "osx_64"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_build_tool": "rattler-build",
    "conda_forge_output_validation": true,
    "conda_install_tool": "pixi",
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "test": "native_and_emulated"
  },
  "feedstock_name": "llama.cpp",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "patches": [
        "mkl.patch"
      ],
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "patches": [
        "mkl.patch"
      ],
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "name": "llama.cpp",
  "osx_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "osx_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": []
    },
    "run": {
      "__set__": true,
      "elements": []
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "osx_arm64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "osx_arm64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": []
    },
    "run": {
      "__set__": true,
      "elements": []
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "outputs_names": {
    "__set__": true,
    "elements": [
      "llama.cpp"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64",
    "osx_64",
    "osx_arm64",
    "win_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/llama.cpp.json"
  },
  "raw_meta_yaml": "context:\n  name: llama.cpp\n  version: \"0.0.2646\"\n  build: 3\n\npackage:\n  name: ${{ name|lower }}\n  version: ${{ version }}\n\nsource:\n  url: https://github.com/ggml-org/${{ name }}/archive/b${{ version | split(\".\") | list | last }}.tar.gz\n  sha256: 02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa\n  patches:\n    - if: blas_impl == \"mkl\"\n      then:\n        - mkl.patch\n\nbuild:\n  number: ${{ build }}\n  string: '{%- if cuda_compiler_version != \"None\" -%}cuda${{ cuda_compiler_version | replace(\".\", \"\") }}_h${{ hash }}_${{ build }}{%- elif (osx and x86_64) or cuda_compiler_version == \"None\" -%}cpu_${{ blas_impl }}_h${{ hash }}_${{ build }}{%- elif osx and arm64 -%}mps_h${{ hash }}_${{ build }}{%- endif -%}'\n\n  script:\n    - if: unix\n      then: |\n        echo hello\n        LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n        {%- macro llama_args(value) %}\n        LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n        {%- endmacro %}\n        {%- macro cmake_args(value) -%}\n        LLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n        {%- endmacro %}\n        ${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n        {%- if osx and arm64 %}\n        ${{ llama_args(\"NATIVE=OFF\") }}\n        ${{ llama_args(\"AVX=OFF\") }}\n        ${{ llama_args(\"AVX2=OFF\") }}\n        ${{ llama_args(\"FMA=OFF\") }}\n        ${{ llama_args(\"F16C=OFF\") }}\n        ${{ llama_args(\"METAL=ON\") }}\n        ${{ llama_args(\"ACCELERATE=ON\") }}\n        {%- endif %}\n        {%- if osx and x86_64 %}\n        ${{ llama_args(\"METAL=OFF\") }}\n        ${{ llama_args(\"ACCELERATE=ON\") }}\n        {%- endif %}\n        {%- if cuda_compiler_version != \"None\" %}\n        ${{ llama_args(\"CUDA=ON\") }}\n        ${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n        {%- endif %}\n        {%- if not osx and cuda_compiler_version == \"None\" %}\n        ${{ llama_args(\"BLAS=ON\") }}\n        {%- if blas_impl == \"mkl\" %}\n        ${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n        {%- endif %}\n        {%- endif %}\n\n        echo $LLAMA_ARGS\n        cmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\n        cmake --build build\n        cmake --install build\n    - if: win\n      then: |\n        echo hello\n        set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF\n        {% macro llama_args(value) -%}\n        set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_${{ value }}\n        {%- endmacro %}\n        {% macro cmake_args(value) -%}\n        set LLAMA_ARGS=%LLAMA_ARGS% -D${{ value }}\n        {%- endmacro %}\n\n        ${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n        ${{ llama_args(\"NATIVE=OFF\") }}\n\n        {%- if cuda_compiler_version == \"None\" %}\n        ${{ llama_args(\"BLAS=ON\") }}\n        {%- if blas_impl == \"mkl\" %}\n        ${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n        {%- endif %}\n        {%- endif %}\n\n        set LLAMA_ARGS\n        cmake -S . -B build -G Ninja %CMAKE_ARGS% %LLAMA_ARGS%\n        cmake --build build\n        cmake --install build\n  dynamic_linking:\n    missing_dso_allowlist:\n      - if: win\n        then:\n          - \"*/nvcuda.dll\"\n      - if: linux\n        then:\n          - \"*/libcuda.so.1\"\n\nrequirements:\n  build:\n    - ${{ compiler('c') }}\n    - ${{ stdlib('c') }}\n    - ${{ compiler('cxx') }}\n    - if: cuda_compiler_version != \"None\"\n      then:\n        - ${{ compiler('cuda') }}\n    - cmake\n    - git\n    - ninja\n    - pkgconfig\n  host:\n    - if: cuda_compiler_version != \"None\"\n      then:\n        # NOTE: Without cuda-version, we are installing cuda-toolkit 11.8 instead of 11.2!\n        - cuda-version ${{ cuda_compiler_version }}.*\n        - if: match(cuda_compiler_version, \"12.*\")\n          then:\n            - cuda-cudart-dev ${{ cuda_compiler_version }}.*\n            - libcublas-dev ${{ cuda_compiler_version }}.*\n\n    - if: (not osx) and (cuda_compiler_version == \"None\") and (blas_impl == \"mkl\")\n      then:\n        - blas-devel * *_${{ blas_impl }}\n        - mkl-devel ${{ mkl }}.*\n  run:\n    - if: cuda_compiler_version != \"None\"\n      then:\n        - cuda-version ${{ cuda_compiler_version }}.*\n        - __cuda\n        - if: match(cuda_compiler_version, \"12.*\")\n          then:\n            - cuda-nvcc-tools\n    - if: linux and (cuda_compiler_vedrsion == \"None\") and (blas_impl == \"mkl\")\n      then:\n        - llvm-openmp\n  run_constraints:\n    # whisper.cpp also vendors ggml\n    - whisper.cpp <0.0.0a0\n\ntests:\n  - if: (build_platform == target_platform) and (cuda_compiler_version == \"None\")\n    then:\n      - script:\n          - main --help\n          - server --help\n        requirements:\n          run:\n            - if: cuda_compiler_version != \"None\"\n              then:\n                - cuda-version ${{ cuda_compiler_version }}\n\nabout:\n  homepage: https://github.com/ggml-org/llama.cpp\n  repository: https://github.com/ggml-org/llama.cpp\n  summary: Port of Facebook's LLaMA model in C/C++\n  license: MIT\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - jjerphan\n    - jonashaag\n    - frankier\n    - sodre\n    - pavelzw\n",
  "req": {
    "__set__": true,
    "elements": [
      "__cuda",
      "blas-devel",
      "c_compiler_stub",
      "c_stdlib_stub",
      "cmake",
      "cuda-cudart-dev",
      "cuda-nvcc-tools",
      "cuda-version",
      "cuda_compiler_stub",
      "cxx_compiler_stub",
      "git",
      "libcublas-dev",
      "mkl-devel",
      "ninja",
      "pkgconfig"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-cudart-dev",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "libcublas-dev",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-nvcc-tools",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel * *_mkl",
        "cuda-cudart-dev 12.9.*",
        "cuda-version 12.9.*",
        "libcublas-dev 12.9.*",
        "mkl-devel 2024.*"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version 12.9.*"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz",
  "version": "0.0.2646",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/llama.cpp.json"
  },
  "win_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "3",
      "script": "echo hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n{%- if osx and arm64 %}\n${{ llama_args(\"NATIVE=OFF\") }}\n${{ llama_args(\"AVX=OFF\") }}\n${{ llama_args(\"AVX2=OFF\") }}\n${{ llama_args(\"FMA=OFF\") }}\n${{ llama_args(\"F16C=OFF\") }}\n${{ llama_args(\"METAL=ON\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if osx and x86_64 %}\n${{ llama_args(\"METAL=OFF\") }}\n${{ llama_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n{%- if cuda_compiler_version != \"None\" %}\n${{ llama_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n{%- if not osx and cuda_compiler_version == \"None\" %}\n${{ llama_args(\"BLAS=ON\") }}\n{%- if blas_impl == \"mkl\" %}\n${{ llama_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [],
        "version": "0.0.2646"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "main --help",
              "server --help"
            ]
          }
        ],
        "version": "0.0.2646"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "0.0.2646"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "patches": [
        "mkl.patch"
      ],
      "sha256": "02953cc03455bac1fcb337ff841600654712f6f6bbed54c31078eead530ff1fa",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b2646.tar.gz"
    }
  },
  "win_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  }
}