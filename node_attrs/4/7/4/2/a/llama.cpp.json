{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "azure": {
      "free_disk_space": true
    },
    "bot": {
      "automerge": true,
      "version_updates": {
        "random_fraction_to_keep": 0.02
      }
    },
    "build_platform": {
      "linux_aarch64": "linux_64",
      "osx_arm64": "osx_64"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_build_tool": "rattler-build",
    "conda_forge_output_validation": true,
    "conda_install_tool": "pixi",
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "test": "native_and_emulated"
  },
  "feedstock_hash": "1a5e25507baa941be38cbf124d581b326cead942",
  "feedstock_hash_ts": 1761850842,
  "feedstock_name": "llama.cpp",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl",
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurl",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl",
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurl"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl",
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "name": "llama.cpp",
  "osx_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "osx_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "libcurl"
      ]
    },
    "run": {
      "__set__": true,
      "elements": []
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "osx_arm64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "osx_arm64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "libcurl"
      ]
    },
    "run": {
      "__set__": true,
      "elements": []
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "outputs_names": {
    "__set__": true,
    "elements": [
      "llama.cpp"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64",
    "osx_64",
    "osx_arm64",
    "win_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/llama.cpp.json"
  },
  "raw_meta_yaml": "context:\n  name: llama.cpp\n  version: \"6800\"\n  build: 1\n\npackage:\n  name: ${{ name|lower }}\n  version: ${{ version }}\n\nsource:\n  url: https://github.com/ggml-org/${{ name }}/archive/b${{ version ~ \"\" | split(\".\") | list | last }}.tar.gz\n  sha256: eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c\n\nbuild:\n  # See https://conda-forge.org/docs/user/tipsandtricks/#installing-cuda-enabled-packages-like-tensorflow-and-pytorch\n  # Similar to https://github.com/conda-forge/jaxlib-feedstock/blob/4471f8d7b0096942f228a9c5a4324bfa75139e22/recipe/recipe.yaml#L22\n  number: ${{ build if cuda_compiler_version == \"None\" else build + 200 }}\n  string: '{%- if cuda_compiler_version != \"None\" -%}cuda${{ cuda_compiler_version | replace(\".\", \"\") }}_h${{ hash }}_${{ build }}{%- elif (osx and x86_64) or cuda_compiler_version == \"None\" -%}cpu_${{ blas_impl }}_h${{ hash }}_${{ build }}{%- elif osx and arm64 -%}mps_h${{ hash }}_${{ build }}{%- endif -%}'\n\n  script:\n    - if: unix\n      then: |\n        # This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n        # https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n        {%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\n        CCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n        {%- endif %}\n\n        {%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\n        CCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n        {%- endif %}\n\n        {%- if match(cuda_compiler_version, \"12.*\") and linux %}\n        ROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\n        patch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n        {%- endif %}\n\n        echo hello\n        LLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n        {%- macro llama_args(value) %}\n        LLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n        {%- endmacro %}\n\n        {%- macro cmake_args(value) -%}\n        LLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n        {%- endmacro %}\n\n        {% macro ggml_args(value) -%}\n        LLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n        {%- endmacro %}\n\n        ${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n        ${{ llama_args(\"CURL=ON\") }}\n\n        {%- if osx and arm64 %}\n        ${{ ggml_args(\"NATIVE=OFF\") }}\n        ${{ ggml_args(\"AVX=OFF\") }}\n        ${{ ggml_args(\"AVX2=OFF\") }}\n        ${{ ggml_args(\"FMA=OFF\") }}\n        ${{ ggml_args(\"F16C=OFF\") }}\n        ${{ ggml_args(\"METAL=ON\") }}\n        ${{ ggml_args(\"ACCELERATE=ON\") }}\n        {%- endif %}\n\n        {%- if osx and x86_64 %}\n        ${{ ggml_args(\"METAL=OFF\") }}\n        ${{ ggml_args(\"ACCELERATE=ON\") }}\n        {%- endif %}\n\n        {%- if cuda_compiler_version != \"None\" %}\n        ${{ ggml_args(\"CUDA=ON\") }}\n        ${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n        {%- endif %}\n\n        {%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n        ${{ ggml_args(\"BLAS=ON\") }}\n\n        {%- if blas_impl == \"mkl\" %}\n        ${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n        {%- endif %}\n\n        {%- endif %}\n\n        echo $LLAMA_ARGS\n        cmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\n        cmake --build build\n        cmake --install build\n    - if: win\n      then: |\n        echo hello\n        set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF\n\n        {% macro llama_args(value) -%}\n        set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_${{ value }}\n        {%- endmacro %}\n\n        {% macro cmake_args(value) -%}\n        set LLAMA_ARGS=%LLAMA_ARGS% -D${{ value }}\n        {%- endmacro %}\n\n        {% macro ggml_args(value) -%}\n        set LLAMA_ARGS=%LLAMA_ARGS% -DGGML_${{ value }}\n        {%- endmacro %}\n\n        ${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n        ${{ llama_args(\"CURL=ON\") }}\n\n        ${{ ggml_args(\"NATIVE=OFF\") }}\n\n        {%- if cuda_compiler_version != \"None\" %}\n        ${{ ggml_args(\"CUDA=ON\") }}\n\n        :: NOTE: is it necessary to set `CMAKE_CUDA_ARCHITECTURES=all`\n        :: on Windows, or is it a nice-to-have?\n        :: ${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n\n        {%- else %}\n\n        ${{ ggml_args(\"BLAS=ON\") }}\n\n        {%- if blas_impl == \"mkl\" %}\n        ${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n        {%- endif %}\n\n        {%- endif %}\n\n        set LLAMA_ARGS\n        cmake -S . -B build -G Ninja %CMAKE_ARGS% %LLAMA_ARGS%\n        cmake --build build\n        cmake --install build\n  dynamic_linking:\n    missing_dso_allowlist:\n      - if: win\n        then:\n          - \"*/nvcuda.dll\"\n      - if: linux\n        then:\n          - \"*/libcuda.so.1\"\n\nrequirements:\n  build:\n    - ${{ compiler('c') }}\n    - ${{ stdlib('c') }}\n    - ${{ compiler('cxx') }}\n    - if: cuda_compiler_version != \"None\"\n      then:\n        - ${{ compiler('cuda') }}\n    - cmake\n    - git\n    - ninja\n    - pkgconfig\n  host:\n    - libcurl\n    - if: cuda_compiler_version != \"None\"\n      then:\n        # NOTE: Without cuda-version, we are installing cuda-toolkit 11.8 instead of 11.2!\n        - cuda-version ${{ cuda_compiler_version }}.*\n        - if: match(cuda_compiler_version, \"12.*\")\n          then:\n            - cuda-cudart-dev ${{ cuda_compiler_version }}.*\n            - libcublas-dev ${{ cuda_compiler_version }}.*\n\n    - if: (not osx) and (cuda_compiler_version == \"None\") and (blas_impl == \"mkl\")\n      then:\n        - blas-devel * *_${{ blas_impl }}\n        - mkl-devel ${{ mkl }}.*\n  run:\n    - if: cuda_compiler_version != \"None\"\n      then:\n        - cuda-version ${{ cuda_compiler_version }}.*\n        - __cuda\n        - if: match(cuda_compiler_version, \"12.*\")\n          then:\n            - cuda-nvcc-tools\n  run_constraints:\n    # whisper.cpp also vendors ggml\n    - whisper.cpp <0.0.0a0\n\ntests:\n  # Test for presence of header files, libraries, and configuration files\n  - if: unix\n    then:\n      - script:\n        # Define function to check file existence with verbose output\n          - |\n            check_file() {\n              local file=\"$1\"\n              local description=\"$2\"\n              if test -f \"$file\"; then\n                echo \"✓ Found: $description\"\n              else\n                echo \"✗ Missing: $description\"\n                return 1\n              fi\n            }\n\n        # Check binaries\n          - echo \"Checking binary files...\"\n          - check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"\n          - check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"\n          - check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"\n          - check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"\n          - check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"\n          - check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"\n          - check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"\n          - check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"\n          - check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"\n          - check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"\n          - check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"\n          - check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"\n          - check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"\n          - check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"\n          - check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"\n          - check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"\n          - check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"\n          - check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"\n          - check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"\n          - check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"\n          - check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"\n          - check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"\n          - check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"\n          - check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"\n          - check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"\n          - check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"\n          - check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"\n          - check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"\n          - check_file \"$PREFIX/bin/llama-run\" \"llama-run\"\n          - check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"\n          - check_file \"$PREFIX/bin/llama-server\" \"llama-server\"\n          - check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"\n          - check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"\n          - check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"\n          - check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"\n          - check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"\n          - check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"\n\n        # Check header files\n          - echo \"Checking header files...\"\n          - check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"\n          - check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"\n          - check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"\n          - check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"\n          - check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"\n          - check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"\n          - check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"\n          - check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"\n          - check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"\n          - check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"\n          - check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"\n          - check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"\n          - check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"\n          - check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"\n          - check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"\n          - check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"\n          - check_file \"$PREFIX/include/llama.h\" \"llama.h\"\n          - check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"\n          - check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"\n\n        # Check CMake configuration files\n          - echo \"Checking CMake configuration files...\"\n          - check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"\n          - check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"\n          - check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"\n          - check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"\n\n        # Check libraries\n          - echo \"Checking library files...\"\n          - check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"\n          - check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"\n          - check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"\n          - check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"\n          - check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"\n\n        # CUDA libraries\n          - if: cuda_compiler_version != \"None\"\n            then:\n              - check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"\n\n        # Packaged everywhere but for CUDA builds and on linux_aarch64\n          - if: (not (linux and aarch64)) and (cuda_compiler_version == \"None\")\n            then:\n              - check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"\n\n        # Check pkg-config file\n          - echo \"Checking pkg-config file...\"\n          - check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\"\n\n  - if: (build_platform == target_platform) and (cuda_compiler_version == \"None\")\n    then:\n      - script:\n          - llama-cli --help\n          - llama-server --help\n          - if: not win\n            then:\n              # Test downloading a smol model (using libcurl) and running it.\n              # TODO: Understand why this fails on Windows.\n              - llama-run smollm:135m\n        requirements:\n          run:\n            - if: cuda_compiler_version != \"None\"\n              then:\n                - cuda-version ${{ cuda_compiler_version }}\n\nabout:\n  homepage: https://github.com/ggml-org/llama.cpp\n  repository: https://github.com/ggml-org/llama.cpp\n  summary: Port of Facebook's LLaMA model in C/C++\n  license: MIT\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - traversaro\n    - jjerphan\n    - jonashaag\n    - frankier\n    - sodre\n    - pavelzw\n",
  "req": {
    "__set__": true,
    "elements": [
      "__cuda",
      "blas-devel",
      "c_compiler_stub",
      "c_stdlib_stub",
      "cmake",
      "cuda-cudart-dev",
      "cuda-nvcc-tools",
      "cuda-version",
      "cuda_compiler_stub",
      "cxx_compiler_stub",
      "git",
      "libcublas-dev",
      "libcurl",
      "mkl-devel",
      "ninja",
      "pkgconfig"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-cudart-dev",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "libcublas-dev",
        "libcurl",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-nvcc-tools",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel * *_mkl",
        "cuda-cudart-dev 12.9.*",
        "cuda-version 12.9.*",
        "libcublas-dev 12.9.*",
        "libcurl",
        "mkl-devel 2024.*"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version 12.9.*"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  },
  "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz",
  "version": "6800",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/llama.cpp.json"
  },
  "win_64_meta_yaml": {
    "about": {
      "dev_url": "https://github.com/ggml-org/llama.cpp",
      "home": "https://github.com/ggml-org/llama.cpp",
      "license": "MIT",
      "license_family": "MIT",
      "license_file": "LICENSE",
      "summary": "Port of Facebook's LLaMA model in C/C++"
    },
    "build": {
      "number": "1",
      "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=all\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n"
    },
    "extra": {
      "recipe-maintainers": [
        "traversaro",
        "jjerphan",
        "jonashaag",
        "frankier",
        "sodre",
        "pavelzw"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "cuda-version 12.9.*",
            "cuda-cudart-dev 12.9.*",
            "libcublas-dev 12.9.*"
          ],
          "run": [
            "cuda-version 12.9.*",
            "__cuda",
            "cuda-nvcc-tools"
          ]
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cuda${SHLIB_EXT}\" \"libggml-cuda${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          }
        ],
        "version": "6800"
      },
      {
        "build": null,
        "name": "llama.cpp",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "c_stdlib_stub",
            "cxx_compiler_stub",
            "cmake",
            "git",
            "ninja",
            "pkgconfig"
          ],
          "host": [
            "libcurl",
            "blas-devel * *_mkl",
            "mkl-devel 2024.*"
          ],
          "run": []
        },
        "tests": [
          {
            "script": [
              "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"✓ Found: $description\"\n  else\n    echo \"✗ Missing: $description\"\n    return 1\n  fi\n}\n",
              "echo \"Checking binary files...\"",
              "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
              "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
              "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
              "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
              "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
              "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
              "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
              "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
              "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
              "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
              "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
              "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
              "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
              "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
              "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
              "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
              "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
              "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
              "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
              "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
              "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
              "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
              "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
              "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
              "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
              "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
              "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
              "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
              "check_file \"$PREFIX/bin/llama-run\" \"llama-run\"",
              "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
              "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
              "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
              "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
              "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
              "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
              "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
              "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
              "echo \"Checking header files...\"",
              "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
              "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
              "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
              "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
              "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
              "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
              "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
              "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
              "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
              "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
              "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
              "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
              "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
              "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
              "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
              "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
              "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
              "check_file \"$PREFIX/include/mtmd-helper.h\" \"mtmd-helper.h\"",
              "check_file \"$PREFIX/include/mtmd.h\" \"mtmd.h\"",
              "echo \"Checking CMake configuration files...\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
              "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
              "echo \"Checking library files...\"",
              "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
              "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
              "echo \"Checking pkg-config file...\"",
              "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
            ]
          },
          {
            "script": [
              "llama-cli --help",
              "llama-server --help",
              "llama-run smollm:135m"
            ]
          }
        ],
        "version": "6800"
      }
    ],
    "package": {
      "name": "llama.cpp",
      "version": "6800"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "cmake",
        "git",
        "ninja",
        "pkgconfig"
      ],
      "host": [
        "libcurl",
        "cuda-version 12.9.*",
        "cuda-cudart-dev 12.9.*",
        "libcublas-dev 12.9.*",
        "blas-devel * *_mkl",
        "mkl-devel 2024.*"
      ],
      "run": [
        "cuda-version 12.9.*",
        "__cuda",
        "cuda-nvcc-tools"
      ]
    },
    "schema_version": 1,
    "source": {
      "sha256": "eaf2a570d19aa19334856589153ac6fe0405a673451c8e1dc88b0f5add5ffc4c",
      "url": "https://github.com/ggml-org/llama.cpp/archive/b6800.tar.gz"
    }
  },
  "win_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "pkgconfig"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "blas-devel",
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurl",
        "mkl-devel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "__cuda",
        "cuda-nvcc-tools",
        "cuda-version"
      ]
    },
    "test": {
      "__set__": true,
      "elements": []
    }
  }
}