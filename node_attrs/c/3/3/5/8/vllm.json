{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "conda_build": {
      "error_overlinking": true
    },
    "conda_build_tool": "rattler-build",
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_name": "vllm",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "Easy, fast, and cheap LLM serving for everyone",
      "doc_url": "https://vllm.readthedocs.io/en/latest/",
      "home": "https://github.com/vllm-project/vllm",
      "license": "Apache-2.0 AND BSD-3-Clause",
      "license_family": "Apache-2.0 AND BSD-3-Clause",
      "license_file": "vllm/LICENSE",
      "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs"
    },
    "build": {
      "number": "0",
      "script": "sed -i.bak 's/set(TORCH_SUPPORTED_VERSION_CUDA \"2.4.0\")/set(TORCH_SUPPORTED_VERSION_CUDA \"${{ pytorch_version }}\")/g' flash-attention/CMakeLists.txt\nexport VLLM_FLASH_ATTN_SRC_DIR=$SRC_DIR/flash-attention\ncd vllm\npython use_existing_torch.py\nmkdir -p $SRC_DIR/vllm/third_party/NVTX/c\nln -s $PREFIX/include $SRC_DIR/vllm/third_party/NVTX/c/include\nexport VERBOSE=1\nexport VLLM_TARGET_DEVICE=${{ vllm_target_device }}\n${{ PYTHON }} -m pip install . -vv --no-build-isolation --no-deps\n"
    },
    "extra": {
      "recipe-maintainers": [
        "maresb",
        "shermansiu"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "six >=1.16.0",
            "setuptools >=74.1.1",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "six >=1.16.0",
            "setuptools >=74.1.1",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      }
    ],
    "package": {
      "name": "vllm",
      "version": "0.8.3"
    },
    "requirements": {
      "build": [
        "cmake >=3.26",
        "git",
        "ninja",
        "zlib",
        "c_stdlib_stub",
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub"
      ],
      "host": [
        "python",
        "jinja2 >=3.1.6",
        "packaging",
        "pip",
        "pytorch ==2.6.0",
        "setuptools >=61",
        "setuptools-scm >=8",
        "wheel",
        "libnuma",
        "pytorch * cuda*",
        "cuda",
        "cuda-cudart-dev",
        "cuda-nvrtc-dev",
        "cuda-nvrtc-static",
        "cuda-version ==12.6",
        "cutlass <4",
        "libcublas-dev",
        "nvtx-c"
      ],
      "run": [
        "python",
        "aiohttp",
        "blake3",
        "cachetools",
        "cloudpickle",
        "compressed-tensors ==0.9.2",
        "depyf ==0.18.0",
        "einops",
        "fastapi >=0.115.0",
        "filelock >=3.16.1",
        "gguf ==0.10.0",
        "importlib-metadata",
        "hf-xet >=0.1.4",
        "huggingface_hub >=0.30.0",
        "lark ==1.2.2",
        "lm-format-enforcer >=0.10.11,<0.11",
        "mistral-common >=1.5.4",
        "msgspec",
        "numpy",
        "openai >=1.52.0",
        "opencv >=4.11.0",
        "outlines ==0.1.11",
        "partial-json-parser",
        "pillow",
        "prometheus_client >=0.18.0",
        "prometheus-fastapi-instrumentator >=7.0.0",
        "protobuf",
        "psutil",
        "py-cpuinfo",
        "pydantic >=2.9",
        "python-json-logger",
        "pytorch ==2.6.0",
        "pyyaml",
        "pyzmq",
        "requests >=2.26.0",
        "scipy",
        "sentencepiece",
        "tiktoken >=0.6.0",
        "tokenizers >=0.19.1",
        "tqdm",
        "transformers >=4.51.0",
        "typing_extensions >=4.10",
        "uvicorn-standard",
        "watchfiles",
        "llguidance >=0.7.9,<0.8.0",
        "xgrammar ==0.1.17",
        "numba ==0.61",
        "ray-cgraph >=2.43.0,!=2.44",
        "torchaudio ==2.6.0",
        "torchvision ==0.21.0",
        "six >=1.16.0",
        "setuptools >=74.1.1",
        "torchaudio",
        "torchvision"
      ]
    },
    "schema_version": 1,
    "source": {
      "patches": [
        "patches/0001-Search-for-the-CUDA-package-in-CMakeLists.patch",
        "patches/0002-Remove-ninja-pip-requirement.patch",
        "patches/0003-Manually-define-gettid.patch"
      ],
      "sha256": "475a39d1093b8ef8a905d63eafe0c6c9b8f4f4c2ae2d23f1f3d0fae5e37bb4bd",
      "url": "https://pypi.org/packages/source/v/vllm/vllm-0.8.3.tar.gz"
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "zlib"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda",
        "cuda-cudart-dev",
        "cuda-nvrtc-dev",
        "cuda-nvrtc-static",
        "cuda-version",
        "cutlass",
        "jinja2",
        "libcublas-dev",
        "libnuma",
        "nvtx-c",
        "packaging",
        "pip",
        "python",
        "pytorch",
        "setuptools",
        "setuptools-scm",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "blake3",
        "cachetools",
        "cloudpickle",
        "compressed-tensors",
        "depyf",
        "einops",
        "fastapi",
        "filelock",
        "gguf",
        "hf-xet",
        "huggingface_hub",
        "importlib-metadata",
        "lark",
        "llguidance",
        "lm-format-enforcer",
        "mistral-common",
        "msgspec",
        "numba",
        "numpy",
        "openai",
        "opencv",
        "outlines",
        "partial-json-parser",
        "pillow",
        "prometheus-fastapi-instrumentator",
        "prometheus_client",
        "protobuf",
        "psutil",
        "py-cpuinfo",
        "pydantic",
        "python",
        "python-json-logger",
        "pytorch",
        "pyyaml",
        "pyzmq",
        "ray-cgraph",
        "requests",
        "scipy",
        "sentencepiece",
        "setuptools",
        "six",
        "tiktoken",
        "tokenizers",
        "torchaudio",
        "torchvision",
        "tqdm",
        "transformers",
        "typing_extensions",
        "uvicorn-standard",
        "watchfiles",
        "xgrammar"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "pytest"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "Easy, fast, and cheap LLM serving for everyone",
      "doc_url": "https://vllm.readthedocs.io/en/latest/",
      "home": "https://github.com/vllm-project/vllm",
      "license": "Apache-2.0 AND BSD-3-Clause",
      "license_family": "Apache-2.0 AND BSD-3-Clause",
      "license_file": "vllm/LICENSE",
      "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs"
    },
    "build": {
      "number": "0",
      "script": "sed -i.bak 's/set(TORCH_SUPPORTED_VERSION_CUDA \"2.4.0\")/set(TORCH_SUPPORTED_VERSION_CUDA \"${{ pytorch_version }}\")/g' flash-attention/CMakeLists.txt\nexport VLLM_FLASH_ATTN_SRC_DIR=$SRC_DIR/flash-attention\ncd vllm\npython use_existing_torch.py\nmkdir -p $SRC_DIR/vllm/third_party/NVTX/c\nln -s $PREFIX/include $SRC_DIR/vllm/third_party/NVTX/c/include\nexport VERBOSE=1\nexport VLLM_TARGET_DEVICE=${{ vllm_target_device }}\n${{ PYTHON }} -m pip install . -vv --no-build-isolation --no-deps\n"
    },
    "extra": {
      "recipe-maintainers": [
        "maresb",
        "shermansiu"
      ]
    },
    "outputs": [
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma",
            "pytorch * cuda*",
            "cuda",
            "cuda-cudart-dev",
            "cuda-nvrtc-dev",
            "cuda-nvrtc-static",
            "cuda-version ==12.6",
            "cutlass <4",
            "libcublas-dev",
            "nvtx-c"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "six >=1.16.0",
            "setuptools >=74.1.1",
            "numba ==0.61",
            "ray-cgraph >=2.43.0,!=2.44",
            "torchaudio ==2.6.0",
            "torchvision ==0.21.0"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm",
                "vllm.vllm_flash_attn"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      },
      {
        "build": null,
        "name": "vllm",
        "requirements": {
          "build": [
            "cmake >=3.26",
            "git",
            "ninja",
            "zlib",
            "c_stdlib_stub",
            "c_compiler_stub",
            "cxx_compiler_stub"
          ],
          "host": [
            "python",
            "jinja2 >=3.1.6",
            "packaging",
            "pip",
            "pytorch ==2.6.0",
            "setuptools >=61",
            "setuptools-scm >=8",
            "wheel",
            "libnuma"
          ],
          "run": [
            "python",
            "aiohttp",
            "blake3",
            "cachetools",
            "cloudpickle",
            "compressed-tensors ==0.9.2",
            "depyf ==0.18.0",
            "einops",
            "fastapi >=0.115.0",
            "filelock >=3.16.1",
            "gguf ==0.10.0",
            "importlib-metadata",
            "hf-xet >=0.1.4",
            "huggingface_hub >=0.30.0",
            "lark ==1.2.2",
            "lm-format-enforcer >=0.10.11,<0.11",
            "mistral-common >=1.5.4",
            "msgspec",
            "numpy",
            "openai >=1.52.0",
            "opencv >=4.11.0",
            "outlines ==0.1.11",
            "partial-json-parser",
            "pillow",
            "prometheus_client >=0.18.0",
            "prometheus-fastapi-instrumentator >=7.0.0",
            "protobuf",
            "psutil",
            "py-cpuinfo",
            "pydantic >=2.9",
            "python-json-logger",
            "pytorch ==2.6.0",
            "pyyaml",
            "pyzmq",
            "requests >=2.26.0",
            "scipy",
            "sentencepiece",
            "tiktoken >=0.6.0",
            "tokenizers >=0.19.1",
            "tqdm",
            "transformers >=4.51.0",
            "typing_extensions >=4.10",
            "uvicorn-standard",
            "watchfiles",
            "llguidance >=0.7.9,<0.8.0",
            "xgrammar ==0.1.17",
            "six >=1.16.0",
            "setuptools >=74.1.1",
            "torchaudio",
            "torchvision"
          ]
        },
        "tests": [
          {
            "python": {
              "imports": [
                "vllm"
              ]
            }
          },
          {
            "script": "vllm --version"
          },
          {
            "files": {
              "source": [
                "vllm/tests"
              ]
            },
            "requirements": {
              "run": [
                "pytest"
              ]
            },
            "script": "pytest ./vllm/tests/core/test_scheduler.py"
          }
        ],
        "version": "0.8.3"
      }
    ],
    "package": {
      "name": "vllm",
      "version": "0.8.3"
    },
    "requirements": {
      "build": [
        "cmake >=3.26",
        "git",
        "ninja",
        "zlib",
        "c_stdlib_stub",
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub"
      ],
      "host": [
        "python",
        "jinja2 >=3.1.6",
        "packaging",
        "pip",
        "pytorch ==2.6.0",
        "setuptools >=61",
        "setuptools-scm >=8",
        "wheel",
        "libnuma",
        "pytorch * cuda*",
        "cuda",
        "cuda-cudart-dev",
        "cuda-nvrtc-dev",
        "cuda-nvrtc-static",
        "cuda-version ==12.6",
        "cutlass <4",
        "libcublas-dev",
        "nvtx-c"
      ],
      "run": [
        "python",
        "aiohttp",
        "blake3",
        "cachetools",
        "cloudpickle",
        "compressed-tensors ==0.9.2",
        "depyf ==0.18.0",
        "einops",
        "fastapi >=0.115.0",
        "filelock >=3.16.1",
        "gguf ==0.10.0",
        "importlib-metadata",
        "hf-xet >=0.1.4",
        "huggingface_hub >=0.30.0",
        "lark ==1.2.2",
        "lm-format-enforcer >=0.10.11,<0.11",
        "mistral-common >=1.5.4",
        "msgspec",
        "numpy",
        "openai >=1.52.0",
        "opencv >=4.11.0",
        "outlines ==0.1.11",
        "partial-json-parser",
        "pillow",
        "prometheus_client >=0.18.0",
        "prometheus-fastapi-instrumentator >=7.0.0",
        "protobuf",
        "psutil",
        "py-cpuinfo",
        "pydantic >=2.9",
        "python-json-logger",
        "pytorch ==2.6.0",
        "pyyaml",
        "pyzmq",
        "requests >=2.26.0",
        "scipy",
        "sentencepiece",
        "tiktoken >=0.6.0",
        "tokenizers >=0.19.1",
        "tqdm",
        "transformers >=4.51.0",
        "typing_extensions >=4.10",
        "uvicorn-standard",
        "watchfiles",
        "llguidance >=0.7.9,<0.8.0",
        "xgrammar ==0.1.17",
        "numba ==0.61",
        "ray-cgraph >=2.43.0,!=2.44",
        "torchaudio ==2.6.0",
        "torchvision ==0.21.0",
        "six >=1.16.0",
        "setuptools >=74.1.1",
        "torchaudio",
        "torchvision"
      ]
    },
    "schema_version": 1,
    "source": {
      "patches": [
        "patches/0001-Search-for-the-CUDA-package-in-CMakeLists.patch",
        "patches/0002-Remove-ninja-pip-requirement.patch",
        "patches/0003-Manually-define-gettid.patch"
      ],
      "sha256": "475a39d1093b8ef8a905d63eafe0c6c9b8f4f4c2ae2d23f1f3d0fae5e37bb4bd",
      "url": "https://pypi.org/packages/source/v/vllm/vllm-0.8.3.tar.gz"
    }
  },
  "name": "vllm",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "vllm"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/vllm.json"
  },
  "raw_meta_yaml": "context:\n  version: 0.8.3\n  pytorch_version: 2.6.0\n  use_cuda: ${{ cuda_compiler_version != \"None\" }}\n  vllm_target_device: ${{ \"cuda\" if use_cuda else \"cpu\" }}\n  is_cross_compiling: ${{ build_platform != target_platform }}\n\npackage:\n  name: vllm\n  version: ${{ version }}\n\nsource:\n- url: https://pypi.org/packages/source/v/vllm/vllm-${{ version }}.tar.gz\n  sha256: 475a39d1093b8ef8a905d63eafe0c6c9b8f4f4c2ae2d23f1f3d0fae5e37bb4bd\n  patches:\n  - patches/0001-Search-for-the-CUDA-package-in-CMakeLists.patch\n  - patches/0002-Remove-ninja-pip-requirement.patch\n  - if: linux\n    then:\n    - patches/0003-Manually-define-gettid.patch\n  - if: is_cross_compiling\n    then:\n    - patches/0004-Factor-in-the-cmake-args-when-building-e.g.-for-cros.patch\n  target_directory: vllm\n# Needs to be vendored because vLLM uses a modified version of the flash attention primitives that supports KV-caching.\n- url: https://github.com/vllm-project/flash-attention/archive/d637d8927a35922ce6f6c0dff6dd3f765ed71f3c.tar.gz\n  sha256: 3099add00c9938735b84319d176c5b239c0165e3f9be6540a7a3505cd897c7cd\n  target_directory: flash-attention\n\nbuild:\n  number: 0\n  script: |\n    sed -i.bak 's/set(TORCH_SUPPORTED_VERSION_CUDA \"2.4.0\")/set(TORCH_SUPPORTED_VERSION_CUDA \"${{ pytorch_version }}\")/g' flash-attention/CMakeLists.txt\n    export VLLM_FLASH_ATTN_SRC_DIR=$SRC_DIR/flash-attention\n    cd vllm\n    python use_existing_torch.py\n    mkdir -p $SRC_DIR/vllm/third_party/NVTX/c\n    ln -s $PREFIX/include $SRC_DIR/vllm/third_party/NVTX/c/include\n    export VERBOSE=1\n    export VLLM_TARGET_DEVICE=${{ vllm_target_device }}\n    ${{ PYTHON }} -m pip install . -vv --no-build-isolation --no-deps\n\n  python:\n    entry_points:\n    - vllm = vllm.entrypoints.cli.main:main\n\n  skip:\n    - win\n    - osx and x86_64\n    # conda-forge torchaudio dropped support for Python 3.9 (llvmlite fix only available for Python >=3.10)\n    - match(python, \"<3.10\")\n\nrequirements:\n  build:\n  - cmake >=3.26\n  - git\n  - ninja\n  - zlib\n  - ${{ stdlib('c') }}\n  - ${{ compiler('c') }}\n  - ${{ compiler('cxx') }}\n  - if: use_cuda\n    then:\n    - ${{ compiler('cuda') }}\n  - if: is_cross_compiling\n    then:\n    - python\n    - cross-python_${{ target_platform }}\n    - pytorch ==${{ pytorch_version }}\n    - if: use_cuda\n      then:\n      - pytorch * [build=cuda*]\n  host:\n  - python\n  - jinja2 >=3.1.6\n  - packaging\n  - pip\n  - pytorch ==${{ pytorch_version }}\n  - setuptools >=61\n  - setuptools-scm >=8\n  - wheel\n  - if: linux\n    then:\n    - libnuma\n  - if: use_cuda\n    then:\n    - pytorch * [build=cuda*]\n    - cuda\n    - cuda-cudart-dev\n    - cuda-nvrtc-dev\n    - cuda-nvrtc-static\n    - cuda-version ==${{ cuda_compiler_version }}\n    - cutlass <4  # Cutlass 4 introduces some major changes to the API that causes it to not compile\n    - libcublas-dev\n    - nvtx-c\n  run:\n  - python\n  - aiohttp\n  - blake3\n  - cachetools\n  - cloudpickle\n  - compressed-tensors ==0.9.2\n  - depyf ==0.18.0\n  - einops\n  - fastapi >=0.115.0\n  - filelock >=3.16.1\n  - gguf ==0.10.0\n  - importlib-metadata\n  - hf-xet >=0.1.4\n  - huggingface_hub >=0.30.0\n  - lark ==1.2.2\n  - lm-format-enforcer >=0.10.11,<0.11\n  - mistral-common >=1.5.4\n  - msgspec\n  - numpy\n  - openai >=1.52.0\n  - opencv >=4.11.0\n  - outlines ==0.1.11\n  - partial-json-parser\n  - pillow\n  - prometheus_client >=0.18.0\n  - prometheus-fastapi-instrumentator >=7.0.0\n  - protobuf\n  - psutil\n  - py-cpuinfo\n  - pydantic >=2.9\n  - python-json-logger\n  - pytorch ==${{ pytorch_version }}\n  - pyyaml\n  - pyzmq\n  - requests >=2.26.0\n  - scipy\n  - sentencepiece\n  - tiktoken >=0.6.0\n  - tokenizers >=0.19.1\n  - tqdm\n  - transformers >=4.51.0\n  - typing_extensions >=4.10\n  - uvicorn-standard\n  - watchfiles\n  - if: x86_64 or arm64 or aarch64\n    then:\n    - llguidance >=0.7.9,< 0.8.0\n  - if: x86_64 or aarch64\n    then:\n    - xgrammar ==0.1.17\n  - if: match(python, \">3.11\")\n    then:\n    - six >=1.16.0\n    - setuptools >=74.1.1\n  - if: use_cuda\n    then:\n    # Numba v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding\n    # Requirements taken from https://github.com/vllm-project/vllm/blob/296c6572dd1f76b31b93be19e550790afcfb8843/requirements/cuda.txt#L4-L5\n    # Also, because conda-forge's torchaudio dropped support for Python 3.9, we don't include the conditional numba 0.60 run requirement.\n    - numba ==0.61\n    - ray-cgraph >=2.43.0,!=2.44\n    - torchaudio ==${{ pytorch_version }}\n    - torchvision ==0.21.0\n    - if: linux64\n      then:\n      - xformers ==0.0.29.post2  # platform_system == \"Linux\" and platform_machine == \"x86_64\"\n    else:\n    - torchaudio\n    - torchvision\n  run_constraints:\n  - if: use_cuda\n    then:\n    - pytorch * [build=cuda*]\n  ignore_run_exports:\n    from_package:\n    - cuda-nvrtc-dev\n    - libcublas-dev\ntests:\n- python:\n    imports:\n    - vllm\n    - if: linux and use_cuda\n      then:\n      - vllm.vllm_flash_attn\n    pip_check: true\n- script:\n  - vllm --version\n- script:\n    # Pick an arbitrary test to run: some of the other ones rely on a bunch of external packages\n  - pytest ./vllm/tests/core/test_scheduler.py\n  requirements:\n    run:\n    - pytest\n  files:\n    source:\n    - vllm/tests\n\nabout:\n  homepage: https://github.com/vllm-project/vllm\n  summary: A high-throughput and memory-efficient inference and serving engine for LLMs\n  description:  Easy, fast, and cheap LLM serving for everyone\n  license: Apache-2.0 AND BSD-3-Clause\n  license_file:\n  - vllm/LICENSE\n  - flash-attention/LICENSE\n  - LICENSE_CUTLASS.txt\n  documentation: https://vllm.readthedocs.io/en/latest/\n\nextra:\n  recipe-maintainers:\n    - maresb\n    - shermansiu\n",
  "req": {
    "__set__": true,
    "elements": [
      "aiohttp",
      "blake3",
      "c_compiler_stub",
      "c_stdlib_stub",
      "cachetools",
      "cloudpickle",
      "cmake",
      "compressed-tensors",
      "cuda",
      "cuda-cudart-dev",
      "cuda-nvrtc-dev",
      "cuda-nvrtc-static",
      "cuda-version",
      "cuda_compiler_stub",
      "cutlass",
      "cxx_compiler_stub",
      "depyf",
      "einops",
      "fastapi",
      "filelock",
      "gguf",
      "git",
      "hf-xet",
      "huggingface_hub",
      "importlib-metadata",
      "jinja2",
      "lark",
      "libcublas-dev",
      "libnuma",
      "llguidance",
      "lm-format-enforcer",
      "mistral-common",
      "msgspec",
      "ninja",
      "numba",
      "numpy",
      "nvtx-c",
      "openai",
      "opencv",
      "outlines",
      "packaging",
      "partial-json-parser",
      "pillow",
      "pip",
      "prometheus-fastapi-instrumentator",
      "prometheus_client",
      "protobuf",
      "psutil",
      "py-cpuinfo",
      "pydantic",
      "python",
      "python-json-logger",
      "pytorch",
      "pyyaml",
      "pyzmq",
      "ray-cgraph",
      "requests",
      "scipy",
      "sentencepiece",
      "setuptools",
      "setuptools-scm",
      "six",
      "tiktoken",
      "tokenizers",
      "torchaudio",
      "torchvision",
      "tqdm",
      "transformers",
      "typing_extensions",
      "uvicorn-standard",
      "watchfiles",
      "wheel",
      "xgrammar",
      "zlib"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "zlib"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda",
        "cuda-cudart-dev",
        "cuda-nvrtc-dev",
        "cuda-nvrtc-static",
        "cuda-version",
        "cuda_compiler_stub",
        "cutlass",
        "cxx_compiler_stub",
        "jinja2",
        "libcublas-dev",
        "libnuma",
        "nvtx-c",
        "packaging",
        "pip",
        "python",
        "pytorch",
        "setuptools",
        "setuptools-scm",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "blake3",
        "c_compiler_stub",
        "c_stdlib_stub",
        "cachetools",
        "cloudpickle",
        "compressed-tensors",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "depyf",
        "einops",
        "fastapi",
        "filelock",
        "gguf",
        "hf-xet",
        "huggingface_hub",
        "importlib-metadata",
        "lark",
        "llguidance",
        "lm-format-enforcer",
        "mistral-common",
        "msgspec",
        "numba",
        "numpy",
        "openai",
        "opencv",
        "outlines",
        "partial-json-parser",
        "pillow",
        "prometheus-fastapi-instrumentator",
        "prometheus_client",
        "protobuf",
        "psutil",
        "py-cpuinfo",
        "pydantic",
        "python",
        "python-json-logger",
        "pytorch",
        "pyyaml",
        "pyzmq",
        "ray-cgraph",
        "requests",
        "scipy",
        "sentencepiece",
        "setuptools",
        "six",
        "tiktoken",
        "tokenizers",
        "torchaudio",
        "torchvision",
        "tqdm",
        "transformers",
        "typing_extensions",
        "uvicorn-standard",
        "watchfiles",
        "xgrammar"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "pytest"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cmake >=3.26",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "git",
        "ninja",
        "zlib"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda",
        "cuda-cudart-dev",
        "cuda-nvrtc-dev",
        "cuda-nvrtc-static",
        "cuda-version ==12.6",
        "cutlass <4",
        "jinja2 >=3.1.6",
        "libcublas-dev",
        "libnuma",
        "nvtx-c",
        "packaging",
        "pip",
        "python",
        "pytorch * cuda*",
        "pytorch ==2.6.0",
        "setuptools >=61",
        "setuptools-scm >=8",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "aiohttp",
        "blake3",
        "cachetools",
        "cloudpickle",
        "compressed-tensors ==0.9.2",
        "depyf ==0.18.0",
        "einops",
        "fastapi >=0.115.0",
        "filelock >=3.16.1",
        "gguf ==0.10.0",
        "hf-xet >=0.1.4",
        "huggingface_hub >=0.30.0",
        "importlib-metadata",
        "lark ==1.2.2",
        "llguidance >=0.7.9,<0.8.0",
        "lm-format-enforcer >=0.10.11,<0.11",
        "mistral-common >=1.5.4",
        "msgspec",
        "numba ==0.61",
        "numpy",
        "openai >=1.52.0",
        "opencv >=4.11.0",
        "outlines ==0.1.11",
        "partial-json-parser",
        "pillow",
        "prometheus-fastapi-instrumentator >=7.0.0",
        "prometheus_client >=0.18.0",
        "protobuf",
        "psutil",
        "py-cpuinfo",
        "pydantic >=2.9",
        "python",
        "python-json-logger",
        "pytorch ==2.6.0",
        "pyyaml",
        "pyzmq",
        "ray-cgraph >=2.43.0,!=2.44",
        "requests >=2.26.0",
        "scipy",
        "sentencepiece",
        "setuptools >=74.1.1",
        "six >=1.16.0",
        "tiktoken >=0.6.0",
        "tokenizers >=0.19.1",
        "torchaudio",
        "torchaudio ==2.6.0",
        "torchvision",
        "torchvision ==0.21.0",
        "tqdm",
        "transformers >=4.51.0",
        "typing_extensions >=4.10",
        "uvicorn-standard",
        "watchfiles",
        "xgrammar ==0.1.17"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "pytest"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/v/vllm/vllm-0.8.3.tar.gz",
  "version": "0.8.3",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/vllm.json"
  }
}