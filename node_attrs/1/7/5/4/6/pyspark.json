{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_.yaml": "channel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\npython_min:\n- '3.10'\n",
  "conda-forge.yml": {
    "conda_build": {
      "pkg_format": "2"
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "provider": {
      "win": "azure"
    }
  },
  "feedstock_hash": "b471b3ad8a2ccbf2bb1edbee394f384b36d467f6",
  "feedstock_hash_ts": 1767979983,
  "feedstock_name": "pyspark",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "Apache Spark is a fast and general engine for large-scale data processing.",
      "home": "http://spark.apache.org/",
      "license": "Apache-2.0",
      "license_file": "/LICENSE",
      "summary": "Apache Spark"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . --no-deps --ignore-installed -vv "
    },
    "extra": {
      "recipe-maintainers": [
        "parente",
        "quasiben",
        "dbast",
        "mariusvniekerk",
        "codesorcery",
        "h-vetinari"
      ]
    },
    "package": {
      "name": "pyspark",
      "version": "4.1.1"
    },
    "requirements": {
      "host": [
        "pip",
        "python 3.10",
        "setuptools"
      ],
      "run": [
        "numpy >=1.21",
        "pandas >=2.0.0",
        "py4j ==0.10.9.9",
        "pyarrow >=11.0.0",
        "python >=3.10"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "77f78984aa84fbe865c717dd37b49913b4e5c97d76ef6824f932f1aefa6621ec",
      "url": "https://dist.apache.org/repos/dist/release/spark/spark-4.1.1/pyspark-4.1.1.tar.gz"
    },
    "test": {
      "commands": [
        "pip check",
        "bash -c \"compgen -c spark && compgen -c pyspark\""
      ],
      "imports": [
        "pyspark",
        "pyspark.cloudpickle",
        "pyspark.ml",
        "pyspark.ml.linalg",
        "pyspark.ml.param",
        "pyspark.mllib",
        "pyspark.mllib.linalg",
        "pyspark.mllib.stat",
        "pyspark.pandas",
        "pyspark.pandas.data_type_ops",
        "pyspark.pandas.indexes",
        "pyspark.pandas.missing",
        "pyspark.pandas.plot",
        "pyspark.pandas.spark",
        "pyspark.pandas.typedef",
        "pyspark.pandas.usage_logging",
        "pyspark.python.pyspark",
        "pyspark.python.lib",
        "pyspark.sql",
        "pyspark.sql.avro",
        "pyspark.sql.pandas",
        "pyspark.streaming",
        "pyspark.bin",
        "pyspark.sbin",
        "pyspark.jars",
        "pyspark.data",
        "pyspark.licenses",
        "pyspark.resource"
      ],
      "requires": [
        "pip",
        "python 3.10"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "numpy",
        "pandas",
        "py4j",
        "pyarrow",
        "python"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "Apache Spark is a fast and general engine for large-scale data processing.",
      "home": "http://spark.apache.org/",
      "license": "Apache-2.0",
      "license_file": "/LICENSE",
      "summary": "Apache Spark"
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . --no-deps --ignore-installed -vv "
    },
    "extra": {
      "recipe-maintainers": [
        "parente",
        "quasiben",
        "dbast",
        "mariusvniekerk",
        "codesorcery",
        "h-vetinari"
      ]
    },
    "package": {
      "name": "pyspark",
      "version": "4.1.1"
    },
    "requirements": {
      "host": [
        "pip",
        "python 3.10",
        "setuptools"
      ],
      "run": [
        "numpy >=1.21",
        "pandas >=2.0.0",
        "py4j ==0.10.9.9",
        "pyarrow >=11.0.0",
        "python >=3.10"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "77f78984aa84fbe865c717dd37b49913b4e5c97d76ef6824f932f1aefa6621ec",
      "url": "https://dist.apache.org/repos/dist/release/spark/spark-4.1.1/pyspark-4.1.1.tar.gz"
    },
    "test": {
      "commands": [
        "pip check",
        "bash -c \"compgen -c spark && compgen -c pyspark\""
      ],
      "imports": [
        "pyspark",
        "pyspark.cloudpickle",
        "pyspark.ml",
        "pyspark.ml.linalg",
        "pyspark.ml.param",
        "pyspark.mllib",
        "pyspark.mllib.linalg",
        "pyspark.mllib.stat",
        "pyspark.pandas",
        "pyspark.pandas.data_type_ops",
        "pyspark.pandas.indexes",
        "pyspark.pandas.missing",
        "pyspark.pandas.plot",
        "pyspark.pandas.spark",
        "pyspark.pandas.typedef",
        "pyspark.pandas.usage_logging",
        "pyspark.python.pyspark",
        "pyspark.python.lib",
        "pyspark.sql",
        "pyspark.sql.avro",
        "pyspark.sql.pandas",
        "pyspark.streaming",
        "pyspark.bin",
        "pyspark.sbin",
        "pyspark.jars",
        "pyspark.data",
        "pyspark.licenses",
        "pyspark.resource"
      ],
      "requires": [
        "pip",
        "python 3.10"
      ]
    }
  },
  "name": "pyspark",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "pyspark"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/pyspark.json"
  },
  "raw_meta_yaml": "{% set version = \"4.1.1\" %}\n\npackage:\n  name: pyspark\n  version: {{ version }}\n\nsource:\n  # PyPI has had issues recently with timely releases due to size constraints of tarball;\n  # Building from source runs into StackOverflow errors in CF CI; --> use upstream binary\n  url: https://dist.apache.org/repos/dist/release/spark/spark-{{ version }}/pyspark-{{ version }}.tar.gz\n  sha256: 77f78984aa84fbe865c717dd37b49913b4e5c97d76ef6824f932f1aefa6621ec\n\nbuild:\n  noarch: python\n  number: 0\n  script: '{{ PYTHON }} -m pip install . --no-deps --ignore-installed -vv '\n\nrequirements:\n  host:\n    - pip\n    - python {{ python_min }}\n    - setuptools\n  run:\n    - numpy >=1.21\n    - pandas >=2.0.0\n    - py4j ==0.10.9.9\n    - pyarrow >=11.0.0\n    - python >={{ python_min }}\n\ntest:\n  commands:\n    - pip check\n    - bash -c \"compgen -c spark && compgen -c pyspark\"  # [not win]\n    - where *spark*                                     # [win]\n  imports:\n    - pyspark\n    - pyspark.cloudpickle\n    - pyspark.ml\n    - pyspark.ml.linalg\n    - pyspark.ml.param\n    - pyspark.mllib\n    - pyspark.mllib.linalg\n    - pyspark.mllib.stat\n    - pyspark.pandas\n    - pyspark.pandas.data_type_ops\n    - pyspark.pandas.indexes\n    - pyspark.pandas.missing\n    - pyspark.pandas.plot\n    - pyspark.pandas.spark\n    - pyspark.pandas.typedef\n    - pyspark.pandas.usage_logging\n    - pyspark.python.pyspark\n    - pyspark.python.lib\n    - pyspark.sql\n    - pyspark.sql.avro\n    - pyspark.sql.pandas\n    - pyspark.streaming\n    - pyspark.bin\n    - pyspark.sbin\n    - pyspark.jars\n    - pyspark.data\n    - pyspark.licenses\n    - pyspark.resource\n  requires:\n    - pip\n    - python {{ python_min }}\n\nabout:\n  home: http://spark.apache.org/\n  license: Apache-2.0\n  # Not yet available in the pypi release\n  license_file: {{ environ[\"RECIPE_DIR\"] }}/LICENSE\n  summary: Apache Spark\n  description: Apache Spark is a fast and general engine for large-scale data processing.\n\nextra:\n  recipe-maintainers:\n    - parente\n    - quasiben\n    - dbast\n    - mariusvniekerk\n    - codesorcery\n    - h-vetinari\n",
  "req": {
    "__set__": true,
    "elements": [
      "numpy",
      "pandas",
      "pip",
      "py4j",
      "pyarrow",
      "python",
      "setuptools"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "numpy",
        "pandas",
        "py4j",
        "pyarrow",
        "python"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.10",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "numpy >=1.21",
        "pandas >=2.0.0",
        "py4j ==0.10.9.9",
        "pyarrow >=11.0.0",
        "python >=3.10"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip",
        "python 3.10"
      ]
    }
  },
  "url": "https://dist.apache.org/repos/dist/release/spark/spark-4.1.1/pyspark-4.1.1.tar.gz",
  "version": "4.1.1",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/pyspark.json"
  }
}