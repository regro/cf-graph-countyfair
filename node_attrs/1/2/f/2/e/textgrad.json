{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_name": "textgrad",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n\n\n\nAn autograd engine -- for textual gradients!\n\nTextGrad is a powerful framework  building automatic ``differentiation'' via text.\nTextGrad implements backpropagation through text feedback provided by LLMs, strongly building on the gradient metaphor\n\nWe provide a simple and intuitive API that allows you to define your own loss functions and optimize them using text feedback.\nThis API is similar to the Pytorch API, making it simple to adapt to your usecases.\n\n![Analogy with Torch](https://github.com/zou-group/textgrad/blob/main/assets/analogy.png?raw=true)\n\nPyPI: [https://pypi.org/project/textgrad/](https://pypi.org/project/textgrad/)\n---\n:fire: The conda-forge recipe was generated with [Conda-Forger App](https://sugatoray-conda-forger.streamlit.app/).\n",
      "dev_url": "https://github.com/zou-group/textgrad",
      "doc_url": "http://textgrad.com/",
      "home": "https://github.com/zou-group/textgrad",
      "license": "MIT",
      "license_file": "LICENSE",
      "summary": "Automatic ''Differentiation'' via Text -- using large language models to backpropagate textual gradients."
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "sugatoray"
      ]
    },
    "package": {
      "name": "textgrad",
      "version": "0.1.4"
    },
    "requirements": {
      "host": [
        "python >=3.9",
        "pip"
      ],
      "run": [
        "python >=3.9",
        "openai >=1.23.6",
        "tenacity >=8.2.3",
        "python-dotenv >=1.0.0",
        "pandas >=1.5.3",
        "platformdirs >=3.11.0",
        "datasets >=2.14.6",
        "diskcache >=5.6.3",
        "python-graphviz >=0.20.3",
        "gdown >=5.2.0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "da4ab8ef17981698fc5345d371fe274393d3b84ee223f743aac09ec58c92581f",
      "url": "https://pypi.io/packages/source/t/textgrad/textgrad-0.1.4.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "textgrad"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "datasets",
        "diskcache",
        "gdown",
        "openai",
        "pandas",
        "platformdirs",
        "python",
        "python-dotenv",
        "python-graphviz",
        "tenacity"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n\n\n\nAn autograd engine -- for textual gradients!\n\nTextGrad is a powerful framework  building automatic ``differentiation'' via text.\nTextGrad implements backpropagation through text feedback provided by LLMs, strongly building on the gradient metaphor\n\nWe provide a simple and intuitive API that allows you to define your own loss functions and optimize them using text feedback.\nThis API is similar to the Pytorch API, making it simple to adapt to your usecases.\n\n![Analogy with Torch](https://github.com/zou-group/textgrad/blob/main/assets/analogy.png?raw=true)\n\nPyPI: [https://pypi.org/project/textgrad/](https://pypi.org/project/textgrad/)\n---\n:fire: The conda-forge recipe was generated with [Conda-Forger App](https://sugatoray-conda-forger.streamlit.app/).\n",
      "dev_url": "https://github.com/zou-group/textgrad",
      "doc_url": "http://textgrad.com/",
      "home": "https://github.com/zou-group/textgrad",
      "license": "MIT",
      "license_file": "LICENSE",
      "summary": "Automatic ''Differentiation'' via Text -- using large language models to backpropagate textual gradients."
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "sugatoray"
      ]
    },
    "package": {
      "name": "textgrad",
      "version": "0.1.4"
    },
    "requirements": {
      "host": [
        "python >=3.9",
        "pip"
      ],
      "run": [
        "python >=3.9",
        "openai >=1.23.6",
        "tenacity >=8.2.3",
        "python-dotenv >=1.0.0",
        "pandas >=1.5.3",
        "platformdirs >=3.11.0",
        "datasets >=2.14.6",
        "diskcache >=5.6.3",
        "python-graphviz >=0.20.3",
        "gdown >=5.2.0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "da4ab8ef17981698fc5345d371fe274393d3b84ee223f743aac09ec58c92581f",
      "url": "https://pypi.io/packages/source/t/textgrad/textgrad-0.1.4.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "textgrad"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "name": "textgrad",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "textgrad"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/textgrad.json"
  },
  "raw_meta_yaml": "{% set name = \"textgrad\" %}\n{% set version = \"0.1.4\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/textgrad-{{ version }}.tar.gz\n  sha256: da4ab8ef17981698fc5345d371fe274393d3b84ee223f743aac09ec58c92581f\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n\nrequirements:\n  host:\n    - python >=3.9\n    - pip\n  run:\n    - python >=3.9\n    - openai >=1.23.6\n    - tenacity >=8.2.3\n    - python-dotenv >=1.0.0\n    - pandas >=1.5.3\n    - platformdirs >=3.11.0\n    - datasets >=2.14.6\n    - diskcache >=5.6.3\n    - python-graphviz >=0.20.3\n    - gdown >=5.2.0\n\ntest:\n  imports:\n    - textgrad\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/zou-group/textgrad\n  summary: \"Automatic ''Differentiation'' via Text -- using large language models to backpropagate textual gradients.\"\n  license: MIT\n  license_file: LICENSE\n  description: |\n    [![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n    \n    ## TextGrad: Automatic ''Differentiation'' via Text\n\n    An autograd engine -- for textual gradients! \n\n    TextGrad is a powerful framework  building automatic ``differentiation'' via text.\n    TextGrad implements backpropagation through text feedback provided by LLMs, strongly building on the gradient metaphor\n\n    We provide a simple and intuitive API that allows you to define your own loss functions and optimize them using text feedback.\n    This API is similar to the Pytorch API, making it simple to adapt to your usecases.\n\n    ![Analogy with Torch](https://github.com/zou-group/textgrad/blob/main/assets/analogy.png?raw=true)\n\n    PyPI: [https://pypi.org/project/{{ name | lower }}/](https://pypi.org/project/{{ name | lower }}/)\n    ---\n    :fire: The conda-forge recipe was generated with [Conda-Forger App](https://sugatoray-conda-forger.streamlit.app/).\n\n  doc_url: http://textgrad.com/\n  dev_url: https://github.com/zou-group/textgrad\n\nextra:\n  recipe-maintainers:\n    - sugatoray\n",
  "req": {
    "__set__": true,
    "elements": [
      "datasets",
      "diskcache",
      "gdown",
      "openai",
      "pandas",
      "pip",
      "platformdirs",
      "python",
      "python-dotenv",
      "python-graphviz",
      "tenacity"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "datasets",
        "diskcache",
        "gdown",
        "openai",
        "pandas",
        "platformdirs",
        "python",
        "python-dotenv",
        "python-graphviz",
        "tenacity"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python >=3.9"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "datasets >=2.14.6",
        "diskcache >=5.6.3",
        "gdown >=5.2.0",
        "openai >=1.23.6",
        "pandas >=1.5.3",
        "platformdirs >=3.11.0",
        "python >=3.9",
        "python-dotenv >=1.0.0",
        "python-graphviz >=0.20.3",
        "tenacity >=8.2.3"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "url": "https://pypi.io/packages/source/t/textgrad/textgrad-0.1.4.tar.gz",
  "version": "0.1.4",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/textgrad.json"
  }
}