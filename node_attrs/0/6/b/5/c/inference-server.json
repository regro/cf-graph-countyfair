{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_.yaml": "cdt_name:\n- conda\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\n",
  "conda-forge.yml": {
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    }
  },
  "feedstock_hash": "7ad2e2e6c18d057aa78ed9a06a9800f877b96000",
  "feedstock_hash_ts": 1732829986,
  "feedstock_name": "inference-server",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "description": "inference-server is a Python library to deploy an AI/ML model to Amazon SageMaker for real-time inference.\nThe library simplifies deploying a model using your own Docker container image.\n",
      "dev_url": "https://github.com/jpmorganchase/inference-server",
      "doc_source_url": "https://github.com/jpmorganchase/inference-server/blob/main/docs/index.rst",
      "doc_url": "https://inference-server.readthedocs.io",
      "home": "https://github.com/jpmorganchase/inference-server",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Deploy your AI/ML model to Amazon SageMaker for Real-Time Inference and Batch Transform using your own Docker container image."
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "faph"
      ]
    },
    "package": {
      "name": "inference-server",
      "version": "1.3.2"
    },
    "requirements": {
      "host": [
        "python >=3.8",
        "setuptools >=45",
        "wheel",
        "setuptools-scm >=6.2",
        "pip"
      ],
      "run": [
        "python >=3.8",
        "botocore",
        "codetiming >=1.4,<2.dev0",
        "orjson >=3.0,<4.dev0",
        "pluggy >=1.0,<2.dev0",
        "werkzeug >=2.0,<3.dev0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "54ff9296d5babcde559480a176fc02418727b909ab2b940a2fc8ef1b40daf00c",
      "url": "https://pypi.org/packages/source/i/inference-server/inference_server-1.3.2.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "inference_server"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools",
        "setuptools-scm",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "botocore",
        "codetiming",
        "orjson",
        "pluggy",
        "python",
        "werkzeug"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "description": "inference-server is a Python library to deploy an AI/ML model to Amazon SageMaker for real-time inference.\nThe library simplifies deploying a model using your own Docker container image.\n",
      "dev_url": "https://github.com/jpmorganchase/inference-server",
      "doc_source_url": "https://github.com/jpmorganchase/inference-server/blob/main/docs/index.rst",
      "doc_url": "https://inference-server.readthedocs.io",
      "home": "https://github.com/jpmorganchase/inference-server",
      "license": "Apache-2.0",
      "license_file": "LICENSE",
      "summary": "Deploy your AI/ML model to Amazon SageMaker for Real-Time Inference and Batch Transform using your own Docker container image."
    },
    "build": {
      "noarch": "python",
      "number": "0",
      "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
    },
    "extra": {
      "recipe-maintainers": [
        "faph"
      ]
    },
    "package": {
      "name": "inference-server",
      "version": "1.3.2"
    },
    "requirements": {
      "host": [
        "python >=3.8",
        "setuptools >=45",
        "wheel",
        "setuptools-scm >=6.2",
        "pip"
      ],
      "run": [
        "python >=3.8",
        "botocore",
        "codetiming >=1.4,<2.dev0",
        "orjson >=3.0,<4.dev0",
        "pluggy >=1.0,<2.dev0",
        "werkzeug >=2.0,<3.dev0"
      ]
    },
    "schema_version": 0,
    "source": {
      "sha256": "54ff9296d5babcde559480a176fc02418727b909ab2b940a2fc8ef1b40daf00c",
      "url": "https://pypi.org/packages/source/i/inference-server/inference_server-1.3.2.tar.gz"
    },
    "test": {
      "commands": [
        "pip check"
      ],
      "imports": [
        "inference_server"
      ],
      "requires": [
        "pip"
      ]
    }
  },
  "name": "inference-server",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "inference-server"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/inference-server.json"
  },
  "raw_meta_yaml": "{% set name = \"inference-server\" %}\n{% set version = \"1.3.2\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/inference_server-{{ version }}.tar.gz\n  sha256: 54ff9296d5babcde559480a176fc02418727b909ab2b940a2fc8ef1b40daf00c\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n  number: 0\n\nrequirements:\n  host:\n    - python >=3.8\n    - setuptools >=45\n    - wheel\n    - setuptools-scm >=6.2\n    - pip\n  run:\n    - python >=3.8\n    - botocore\n    - codetiming >=1.4,<2.dev0\n    - orjson >=3.0,<4.dev0\n    - pluggy >=1.0,<2.dev0\n    - werkzeug >=2.0,<3.dev0\n\ntest:\n  imports:\n    - inference_server\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  summary: Deploy your AI/ML model to Amazon SageMaker for Real-Time Inference and Batch Transform using your own Docker container image.\n  description: |\n    inference-server is a Python library to deploy an AI/ML model to Amazon SageMaker for real-time inference. \n    The library simplifies deploying a model using your own Docker container image.\n  home: https://github.com/jpmorganchase/inference-server\n  dev_url: https://github.com/jpmorganchase/inference-server\n  doc_url: https://inference-server.readthedocs.io\n  doc_source_url: https://github.com/jpmorganchase/inference-server/blob/main/docs/index.rst\n  license: Apache-2.0\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - faph\n",
  "req": {
    "__set__": true,
    "elements": [
      "botocore",
      "codetiming",
      "orjson",
      "pip",
      "pluggy",
      "python",
      "setuptools",
      "setuptools-scm",
      "werkzeug",
      "wheel"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python",
        "setuptools",
        "setuptools-scm",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "botocore",
        "codetiming",
        "orjson",
        "pluggy",
        "python",
        "werkzeug"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": []
    },
    "host": {
      "__set__": true,
      "elements": [
        "pip",
        "python >=3.8",
        "setuptools >=45",
        "setuptools-scm >=6.2",
        "wheel"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "botocore",
        "codetiming >=1.4,<2.dev0",
        "orjson >=3.0,<4.dev0",
        "pluggy >=1.0,<2.dev0",
        "python >=3.8",
        "werkzeug >=2.0,<3.dev0"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/i/inference-server/inference_server-1.3.2.tar.gz",
  "version": "1.3.2",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/inference-server.json"
  }
}