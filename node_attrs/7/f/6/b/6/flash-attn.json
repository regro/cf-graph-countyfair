{
  "archived": false,
  "branch": "main",
  "conda-forge.yml": {
    "build_platform": {
      "linux_aarch64": "linux_64"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "github_actions": {
      "self_hosted": true,
      "timeout_minutes": 1080,
      "triggers": [
        "push",
        "pull_request"
      ]
    },
    "provider": {
      "linux_64": "github_actions",
      "linux_aarch64": "github_actions"
    }
  },
  "feedstock_name": "flash-attn",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "0",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.2"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "0",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.2"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "0",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.2.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "libtorch",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.2"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "name": "flash-attn-split",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "flash-attn",
      "flash-attn-fused-dense",
      "flash-attn-layer-norm"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/flash-attn.json"
  },
  "raw_meta_yaml": "{% set version = \"2.8.2\" %}\n\npackage:\n  name: flash-attn-split\n  # Strip \".postX\" from version number because these just indicate wheel rebuilds and\n  # are not relevant to conda builds\n  version: {{ version.split('.')[:3] | join('.') }}\n\nsource:\n  # Use PYPI sdist instead of GitHub because they include a specific revended CUTLASS\n  - url: https://pypi.org/packages/source/f/flash-attn/flash_attn-{{ version }}.tar.gz\n    sha256: 740a5370f406cbe16155cc6d078ec543a97a137501f32cba89198295e6b80e54\n  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries\n  - path: pyproject.toml\n  - path: setup.py\n\nbuild:\n  number: 0\n  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation\n  script_env:\n    # Limit MAX_JOBS in order to prevent runners from crashing\n    - MAX_JOBS=4\n    - TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX\n  # Satisfy linter by not having duplicate skip keys\n  skip: true  # [cuda_compiler_version == \"None\" or (not linux)]\n  # debugging skips below\n  # skip: true  # [py!=313]\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}\n    - {{ stdlib('c') }}\n    - ninja\n    - pytorch           # [build_platform != target_platform]\n    - pytorch * cuda*   # [build_platform != target_platform]\n  host:\n    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n    - cuda-cudart-dev\n    - libcublas-dev\n    - libcurand-dev\n    - libcusolver-dev\n    - libcusparse-dev\n    - libtorch         # required until pytorch run_exports libtorch\n    - pip\n    - python\n    - pytorch\n    - pytorch * cuda*\n    - setuptools\n\noutputs:\n  - name: flash-attn\n    requirements:\n      build:\n        - {{ compiler('c') }}\n        - {{ compiler('cxx') }}\n        - {{ compiler('cuda') }}\n        - {{ stdlib('c') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - python\n        - libtorch         # required until pytorch run_exports libtorch\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - einops\n        - python\n        - pytorch * cuda*\n\n    files:\n      include:\n        - 'lib/python*/site-packages/flash_attn/**'\n        - 'lib/python*/site-packages/flash_attn-{{ version }}.dist-info/**'\n        - 'lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-fused-dense\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - libcublas-dev\n        - python\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/fused_dense_lib.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.fused_dense\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-layer-norm\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - python\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/dropout_layer_norm.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.layer_norm\n      commands:\n        - pip check\n      requires:\n        - pip\n\nabout:\n  home: https://github.com/Dao-AILab/flash-attention\n  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'\n  license: BSD-3-Clause\n  license_file:\n    - LICENSE\n    - LICENSE_CUTLASS.txt\n\nextra:\n  feedstock-name: flash-attn\n  recipe-maintainers:\n    - carterbox\n    - weiji14\n",
  "req": {
    "__set__": true,
    "elements": [
      "c_compiler_stub",
      "c_stdlib_stub",
      "cuda-cudart-dev",
      "cuda-version",
      "cuda_compiler_stub",
      "cxx_compiler_stub",
      "einops",
      "flash-attn",
      "libcublas-dev",
      "libcurand-dev",
      "libcusolver-dev",
      "libcusparse-dev",
      "libtorch",
      "ninja",
      "pip",
      "python",
      "pytorch",
      "setuptools"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-cudart-dev",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version 12.9",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "libtorch",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch * cuda*"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.2.tar.gz",
  "version": "2.8.2",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/flash-attn.json"
  }
}