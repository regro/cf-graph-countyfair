{
  "archived": false,
  "branch": "main",
  "ci_support_linux_64_c_stdlib_version2.17cuda_compiler_version12.9python3.10.____cpython.yaml": "c_compiler:\n- gcc\nc_compiler_version:\n- '14'\nc_stdlib:\n- sysroot\nc_stdlib_version:\n- '2.17'\nchannel_sources:\n- conda-forge\nchannel_targets:\n- conda-forge main\ncuda_compiler:\n- cuda-nvcc\ncuda_compiler_version:\n- '12.9'\ncxx_compiler:\n- gxx\ncxx_compiler_version:\n- '14'\ndocker_image:\n- quay.io/condaforge/linux-anvil-x86_64:alma9\ngithub_actions_labels:\n- cirun-openstack-cpu-xlarge\npin_run_as_build:\n  python:\n    min_pin: x.x\n    max_pin: x.x\npython:\n- 3.10.* *_cpython\npytorch:\n- '2.10'\ntarget_platform:\n- linux-64\nzip_keys:\n- - c_compiler_version\n  - cxx_compiler_version\n  - c_stdlib_version\n  - cuda_compiler_version\n",
  "conda-forge.yml": {
    "build_platform": {
      "linux_aarch64": "linux_64"
    },
    "conda_build": {
      "error_overlinking": true
    },
    "conda_forge_output_validation": true,
    "github": {
      "branch_name": "main",
      "tooling_branch_name": "main"
    },
    "github_actions": {
      "self_hosted": true,
      "timeout_minutes": 1080,
      "triggers": [
        "push",
        "pull_request"
      ]
    },
    "provider": {
      "linux_64": "github_actions",
      "linux_aarch64": "github_actions"
    }
  },
  "feedstock_hash": "c38687c36dbb6ab0bca982853f321adeba59f050",
  "feedstock_hash_ts": 1770030802,
  "feedstock_name": "flash-attn",
  "hash_type": "sha256",
  "linux_64_meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "3",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.3"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools",
        "cuda-version 13.0"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "linux_64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "linux_aarch64_meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "3",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.3"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools",
        "cuda-version 13.0"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "linux_aarch64_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "meta_yaml": {
    "about": {
      "home": "https://github.com/Dao-AILab/flash-attention",
      "license": "BSD-3-Clause",
      "license_file": [
        "LICENSE",
        "LICENSE_CUTLASS.txt"
      ],
      "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
    },
    "build": {
      "number": "3",
      "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
      "script_env": [
        "MAX_JOBS=4",
        "TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX"
      ]
    },
    "extra": {
      "feedstock-name": "flash-attn",
      "recipe-maintainers": [
        "carterbox",
        "weiji14"
      ]
    },
    "outputs": [
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 12.9",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/flash_attn/**",
            "lib/python*/site-packages/flash_attn-2.8.3.dist-info/**",
            "lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so"
          ]
        },
        "name": "flash-attn",
        "requirements": {
          "build": [
            "c_compiler_stub",
            "cxx_compiler_stub",
            "cuda_compiler_stub",
            "c_stdlib_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "einops",
            "python",
            "pytorch * cuda*"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/fused_dense_lib.cpython-*.so"
          ]
        },
        "name": "flash-attn-fused-dense",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "libcublas-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.fused_dense"
          ],
          "requires": [
            "pip"
          ]
        }
      },
      {
        "files": {
          "include": [
            "lib/python*/site-packages/dropout_layer_norm.cpython-*.so"
          ]
        },
        "name": "flash-attn-layer-norm",
        "requirements": {
          "build": [
            "cxx_compiler_stub",
            "c_stdlib_stub",
            "cuda_compiler_stub"
          ],
          "host": [
            "cuda-version 13.0",
            "cuda-cudart-dev",
            "python",
            "pytorch",
            "pytorch * cuda*"
          ],
          "run": [
            "python",
            "flash-attn"
          ]
        },
        "test": {
          "commands": [
            "pip check"
          ],
          "imports": [
            "flash_attn.ops.layer_norm"
          ],
          "requires": [
            "pip"
          ]
        }
      }
    ],
    "package": {
      "name": "flash-attn-split",
      "version": "2.8.3"
    },
    "requirements": {
      "build": [
        "c_compiler_stub",
        "cxx_compiler_stub",
        "cuda_compiler_stub",
        "c_stdlib_stub",
        "ninja"
      ],
      "host": [
        "cuda-version 12.9",
        "cuda-cudart-dev",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools",
        "cuda-version 13.0"
      ]
    },
    "schema_version": 0,
    "source": [
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      },
      {
        "sha256": "1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d",
        "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz"
      },
      {
        "path": "pyproject.toml"
      },
      {
        "path": "setup.py"
      }
    ]
  },
  "name": "flash-attn-split",
  "outputs_names": {
    "__set__": true,
    "elements": [
      "flash-attn",
      "flash-attn-fused-dense",
      "flash-attn-layer-norm"
    ]
  },
  "parsing_error": false,
  "platforms": [
    "linux_64",
    "linux_aarch64"
  ],
  "pr_info": {
    "__lazy_json__": "pr_info/flash-attn.json"
  },
  "raw_meta_yaml": "{% set version = \"2.8.3\" %}\n\npackage:\n  name: flash-attn-split\n  # Strip \".postX\" from version number because these just indicate wheel rebuilds and\n  # are not relevant to conda builds\n  version: {{ version.split('.')[:3] | join('.') }}\n\nsource:\n  # Use PYPI sdist instead of GitHub because they include a specific revended CUTLASS\n  - url: https://pypi.org/packages/source/f/flash-attn/flash_attn-{{ version }}.tar.gz\n    sha256: 1e71dd64a9e0280e0447b8a0c2541bad4bf6ac65bdeaa2f90e51a9e57de0370d\n  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries\n  - path: pyproject.toml\n  - path: setup.py\n\nbuild:\n  number: 3\n  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation\n  script_env:\n    # Limit MAX_JOBS in order to prevent runners from crashing\n    - MAX_JOBS=4\n    - TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;10.0;12.0+PTX\n  # Satisfy linter by not having duplicate skip keys\n  skip: true  # [cuda_compiler_version == \"None\" or (not linux)]\n  # debugging skips below\n  # skip: true  # [py!=313]\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}\n    - {{ stdlib('c') }}\n    - ninja\n    - pytorch           # [build_platform != target_platform]\n    - pytorch * cuda*   # [build_platform != target_platform]\n  host:\n    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n    - cuda-cudart-dev\n    - libcublas-dev\n    - libcurand-dev\n    - libcusolver-dev\n    - libcusparse-dev\n    - pip\n    - python\n    - pytorch\n    - pytorch * cuda*\n    - setuptools\n\noutputs:\n  - name: flash-attn\n    requirements:\n      build:\n        - {{ compiler('c') }}\n        - {{ compiler('cxx') }}\n        - {{ compiler('cuda') }}\n        - {{ stdlib('c') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - python\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - einops\n        - python\n        - pytorch * cuda*\n\n    files:\n      include:\n        - 'lib/python*/site-packages/flash_attn/**'\n        - 'lib/python*/site-packages/flash_attn-{{ version }}.dist-info/**'\n        - 'lib/python*/site-packages/flash_attn_2_cuda.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-fused-dense\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - libcublas-dev\n        - python\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/fused_dense_lib.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.fused_dense\n      commands:\n        - pip check\n      requires:\n        - pip\n\n  - name: flash-attn-layer-norm\n    requirements:\n      build:\n        - {{ compiler('cxx') }} # needed for DSO checker\n        - {{ stdlib('c') }}     # needed for DSO checker\n        - {{ compiler('cuda') }}\n      host:\n        - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n        - cuda-cudart-dev\n        - python\n        - pytorch\n        - pytorch * cuda*\n      run:\n        - python\n        - {{ pin_subpackage('flash-attn', exact=True) }}\n\n    files:\n      include:\n        - 'lib/python*/site-packages/dropout_layer_norm.cpython-*.so'\n\n    test:\n      imports:\n        - flash_attn.ops.layer_norm\n      commands:\n        - pip check\n      requires:\n        - pip\n\nabout:\n  home: https://github.com/Dao-AILab/flash-attention\n  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'\n  license: BSD-3-Clause\n  license_file:\n    - LICENSE\n    - LICENSE_CUTLASS.txt\n\nextra:\n  feedstock-name: flash-attn\n  recipe-maintainers:\n    - carterbox\n    - weiji14\n",
  "req": {
    "__set__": true,
    "elements": [
      "c_compiler_stub",
      "c_stdlib_stub",
      "cuda-cudart-dev",
      "cuda-version",
      "cuda_compiler_stub",
      "cxx_compiler_stub",
      "einops",
      "flash-attn",
      "libcublas-dev",
      "libcurand-dev",
      "libcusolver-dev",
      "libcusparse-dev",
      "ninja",
      "pip",
      "python",
      "pytorch",
      "setuptools"
    ]
  },
  "requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda-cudart-dev",
        "cuda-version",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "einops",
        "flash-attn",
        "python",
        "pytorch"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "strong_exports": false,
  "total_requirements": {
    "build": {
      "__set__": true,
      "elements": [
        "c_compiler_stub",
        "c_stdlib_stub",
        "cuda_compiler_stub",
        "cxx_compiler_stub",
        "ninja"
      ]
    },
    "host": {
      "__set__": true,
      "elements": [
        "cuda-cudart-dev",
        "cuda-version 12.9",
        "cuda-version 13.0",
        "libcublas-dev",
        "libcurand-dev",
        "libcusolver-dev",
        "libcusparse-dev",
        "pip",
        "python",
        "pytorch",
        "pytorch * cuda*",
        "setuptools"
      ]
    },
    "run": {
      "__set__": true,
      "elements": [
        "einops",
        "flash-attn",
        "python",
        "pytorch * cuda*"
      ]
    },
    "test": {
      "__set__": true,
      "elements": [
        "pip"
      ]
    }
  },
  "url": "https://pypi.org/packages/source/f/flash-attn/flash_attn-2.8.3.tar.gz",
  "version": "2.8.3",
  "version_pr_info": {
    "__lazy_json__": "version_pr_info/flash-attn.json"
  }
}